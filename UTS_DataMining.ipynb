{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunita-wjy/data-mining/blob/main/UTS_DataMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TensorFlow\n",
        "Framework open-source dari Google yang digunakan untuk membuat dan menjalankan model machine learning dan deep learning.\n",
        "Fungsinya membantu komputer belajar dari data agar bisa menemukan pola, memprediksi, dan mengambil keputusan tanpa perlu diprogram manual.\n",
        "\n",
        "**Kegunaan dalam data mining**: TensorFlow memproses data dalam bentuk tensor, melakukan perhitungan matematis, dan membangun model yang bisa mempelajari pola dari data.\n",
        "\n",
        "*Tensor* : struktur data multidimensi (mirip array atau matriks).\n",
        "\n",
        "TensorFlow membantu di tahap **analisis prediktif dan modeling** menggunakan teknik *machine learning, deep learning*"
      ],
      "metadata": {
        "id": "a4ooLeRwcGTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kelebihan\n",
        "\n",
        "**Performa Tinggi (GPU & TPU Acceleration)**\n",
        "\n",
        "TensorFlow bisa jalan di CPU, GPU, bahkan TPU (Tensor Processing Unit) — hardware khusus dari Google buat deep learning.\n",
        "Untuk dataset besar (jutaan baris), TensorFlow bisa menyelesaikan model training lebih cepat, sehingga eksplorasi pola data jadi lebih efisien.\n",
        "\n",
        "**Pipeline Data Otomatis dan Scalable**\n",
        "\n",
        "Dengan modul tf.data, TensorFlow bisa langsung membaca data dari CSV, database, atau big data storage, lalu membuat pipeline otomatis (load → batch → shuffle → train).\n",
        "Hal ini mempercepat proses data preparation sebelum mining."
      ],
      "metadata": {
        "id": "QAdt5j4YJM59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Depedencies"
      ],
      "metadata": {
        "id": "K7hAGHm_uWSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmKh8DnIuRjz",
        "outputId": "5722b049-d823-47d6-f3a6-14dbdb5addae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Aplikasi penggunaan"
      ],
      "metadata": {
        "id": "kbVms8wg30E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Prediksi hubungan linear – Regresi\n"
      ],
      "metadata": {
        "id": "cjer8zZ2v2EM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAT0LUnOL8Jt",
        "outputId": "e2bdd898-c7e2-4e41-bd19-8b1ef4627bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57.2137   \n",
            "Epoch 2/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4308\n",
            "Epoch 3/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1412\n",
            "Epoch 4/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0199    \n",
            "Epoch 5/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0414\n",
            "Epoch 6/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 7/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0227\n",
            "Epoch 8/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0282\n",
            "Epoch 9/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0249\n",
            "Epoch 10/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0273\n",
            "Epoch 11/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0200\n",
            "Epoch 12/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186\n",
            "Epoch 13/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0262\n",
            "Epoch 14/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0138    \n",
            "Epoch 15/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0321\n",
            "Epoch 16/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0232    \n",
            "Epoch 17/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0157    \n",
            "Epoch 18/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193\n",
            "Epoch 19/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0168    \n",
            "Epoch 20/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0273\n",
            "Epoch 21/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0161\n",
            "Epoch 22/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185\n",
            "Epoch 23/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0250\n",
            "Epoch 24/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0208\n",
            "Epoch 25/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0123    \n",
            "Epoch 26/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0099\n",
            "Epoch 27/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0169\n",
            "Epoch 28/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0105\n",
            "Epoch 29/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0158\n",
            "Epoch 30/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0130    \n",
            "Epoch 31/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0140\n",
            "Epoch 32/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0101\n",
            "Epoch 33/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0161\n",
            "Epoch 34/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0088\n",
            "Epoch 35/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0081\n",
            "Epoch 36/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0136\n",
            "Epoch 37/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0164\n",
            "Epoch 38/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0145\n",
            "Epoch 39/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0064    \n",
            "Epoch 40/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0064\n",
            "Epoch 41/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110\n",
            "Epoch 42/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0135\n",
            "Epoch 43/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0065\n",
            "Epoch 44/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0065    \n",
            "Epoch 45/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0086\n",
            "Epoch 46/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0072    \n",
            "Epoch 47/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0062\n",
            "Epoch 48/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0065\n",
            "Epoch 49/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0076\n",
            "Epoch 50/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042\n",
            "Epoch 51/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045    \n",
            "Epoch 52/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0059\n",
            "Epoch 53/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0054\n",
            "Epoch 54/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058    \n",
            "Epoch 55/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037\n",
            "Epoch 56/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0056    \n",
            "Epoch 57/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0055\n",
            "Epoch 58/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0075\n",
            "Epoch 59/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043\n",
            "Epoch 60/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042    \n",
            "Epoch 61/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0055\n",
            "Epoch 62/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0064\n",
            "Epoch 63/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049\n",
            "Epoch 64/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022    \n",
            "Epoch 65/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024    \n",
            "Epoch 66/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0051\n",
            "Epoch 67/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0056 \n",
            "Epoch 68/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027    \n",
            "Epoch 69/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031    \n",
            "Epoch 70/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028    \n",
            "Epoch 71/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020    \n",
            "Epoch 72/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042\n",
            "Epoch 73/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022    \n",
            "Epoch 74/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045\n",
            "Epoch 75/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0032\n",
            "Epoch 76/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022    \n",
            "Epoch 77/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038\n",
            "Epoch 78/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020    \n",
            "Epoch 79/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016    \n",
            "Epoch 80/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023    \n",
            "Epoch 81/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0015     \n",
            "Epoch 82/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022     \n",
            "Epoch 83/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020     \n",
            "Epoch 84/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0025\n",
            "Epoch 85/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0010    \n",
            "Epoch 86/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023\n",
            "Epoch 87/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 88/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028\n",
            "Epoch 89/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0017    \n",
            "Epoch 90/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
            "Epoch 91/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0011    \n",
            "Epoch 92/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0010    \n",
            "Epoch 93/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1140e-04\n",
            "Epoch 94/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0796e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0438e-04\n",
            "Epoch 96/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021\n",
            "Epoch 97/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6625e-04\n",
            "Epoch 98/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3242e-04\n",
            "Epoch 99/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016\n",
            "Epoch 100/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0012\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "y =  [[21.107437]]\n"
          ]
        }
      ],
      "source": [
        "# Data: y = 2x + 1\n",
        "x_train = [1, 2, 3, 4, 5]\n",
        "y_train = [3, 5, 7, 9, 11]\n",
        "\n",
        "# Buat dataset dari (x, y)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "# Optional: atur agar data diacak dan dibagi batch\n",
        "dataset = dataset.shuffle(buffer_size=5).batch(1)\n",
        "\n",
        "# Model regresi\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, input_shape=[1])\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error') #setting ML\n",
        "model.fit(dataset, epochs=100, verbose=1) #training\n",
        "\n",
        "# Prediksi: x = 10 → y = ?\n",
        "y = model.predict(np.array([[10.0]]))  # Harusnya = 21.0\n",
        "print(\"y = \", y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV langsung jadi dataset TensorFlow\n",
        "dataset = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern='all_seasons.csv',\n",
        "    batch_size=8,\n",
        "    label_name='weight',     # kolom target\n",
        "    num_epochs=1,\n",
        "    shuffle=True,\n",
        "    ignore_errors=True       # biar gak crash kalau ada baris aneh\n",
        ")\n",
        "\n",
        "# Ambil hanya kolom numeric yang mau dipakai (misal 'height')\n",
        "def pack_features(x, y):\n",
        "    # Pastikan height di-cast ke float dan bentuknya (batch, 1)\n",
        "    height = tf.cast(x['height'], tf.float32)\n",
        "    height = tf.reshape(height, [-1, 1])\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    return height, y\n",
        "\n",
        "dataset = dataset.map(pack_features)\n",
        "\n",
        "# Normalisasi fitur\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "height_ds = dataset.map(lambda x, y: x)\n",
        "# Pastikan semua tensor dikonversi ke float\n",
        "height_ds = height_ds.map(lambda x: tf.cast(x, tf.float32))\n",
        "\n",
        "# Jalankan adapt — ini bagian yang dulu error\n",
        "normalizer.adapt(height_ds)\n",
        "\n",
        "# Bangun model sederhana\n",
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss='mean_squared_error')\n",
        "\n",
        "# Training\n",
        "model.fit(dataset, epochs=100, verbose=1)\n",
        "\n",
        "# Prediksi contoh input\n",
        "height_input = 195\n",
        "pred = model.predict(tf.constant([[height_input]], dtype=tf.float32))\n",
        "print(f\"Prediksi berat badan untuk tinggi {height_input} cm: {pred[0][0]:.2f} kg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXj4dimIlheZ",
        "outputId": "1f3f8944-e0f1-489a-a7e6-2f1d7b03e713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 6535.4482\n",
            "Epoch 2/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 98.0855\n",
            "Epoch 3/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 58.5073\n",
            "Epoch 4/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.0493\n",
            "Epoch 5/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.4834\n",
            "Epoch 6/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 49.5510\n",
            "Epoch 7/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.5383\n",
            "Epoch 8/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6047\n",
            "Epoch 9/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.2751\n",
            "Epoch 10/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6589\n",
            "Epoch 11/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.8314\n",
            "Epoch 12/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.1302\n",
            "Epoch 13/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.4041\n",
            "Epoch 14/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.0949\n",
            "Epoch 15/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.9386\n",
            "Epoch 16/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2574\n",
            "Epoch 17/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6275\n",
            "Epoch 18/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.7933\n",
            "Epoch 19/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 48.6885\n",
            "Epoch 20/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.9264\n",
            "Epoch 21/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 48.5116\n",
            "Epoch 22/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.4284\n",
            "Epoch 23/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.3679\n",
            "Epoch 24/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.4738\n",
            "Epoch 25/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.2562\n",
            "Epoch 26/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 49.7088\n",
            "Epoch 27/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.4938\n",
            "Epoch 28/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.0867\n",
            "Epoch 29/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.1482\n",
            "Epoch 30/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 49.6027\n",
            "Epoch 31/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.8730\n",
            "Epoch 32/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 49.8935\n",
            "Epoch 33/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.0458\n",
            "Epoch 34/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 49.8627\n",
            "Epoch 35/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.8227\n",
            "Epoch 36/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 47.6511\n",
            "Epoch 37/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.3728\n",
            "Epoch 38/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.1874\n",
            "Epoch 39/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.8461\n",
            "Epoch 40/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.7059\n",
            "Epoch 41/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6343\n",
            "Epoch 42/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 51.2870\n",
            "Epoch 43/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 48.0621\n",
            "Epoch 44/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.9687\n",
            "Epoch 45/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.1424\n",
            "Epoch 46/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2539\n",
            "Epoch 47/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.3030\n",
            "Epoch 48/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.5599\n",
            "Epoch 49/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.3353\n",
            "Epoch 50/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.1704\n",
            "Epoch 51/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2419\n",
            "Epoch 52/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 50.1041\n",
            "Epoch 53/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2664\n",
            "Epoch 54/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.4647\n",
            "Epoch 55/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.6739\n",
            "Epoch 56/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 48.4330\n",
            "Epoch 57/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.2179\n",
            "Epoch 58/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2561\n",
            "Epoch 59/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.1059\n",
            "Epoch 60/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6974\n",
            "Epoch 61/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.4814\n",
            "Epoch 62/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.1087\n",
            "Epoch 63/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.3664\n",
            "Epoch 64/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.5396\n",
            "Epoch 65/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 48.3160\n",
            "Epoch 66/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2324\n",
            "Epoch 67/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.2159\n",
            "Epoch 68/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.1693\n",
            "Epoch 69/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 50.0796\n",
            "Epoch 70/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.3404\n",
            "Epoch 71/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.8669\n",
            "Epoch 72/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.9058\n",
            "Epoch 73/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.5286\n",
            "Epoch 74/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.2956\n",
            "Epoch 75/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50.3749\n",
            "Epoch 76/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.0967\n",
            "Epoch 77/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.5172\n",
            "Epoch 78/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 49.3487\n",
            "Epoch 79/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.9632\n",
            "Epoch 80/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.6695\n",
            "Epoch 81/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.8370\n",
            "Epoch 82/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 47.6814\n",
            "Epoch 83/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.2647\n",
            "Epoch 84/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.8596\n",
            "Epoch 85/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.4845\n",
            "Epoch 86/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 50.0935\n",
            "Epoch 87/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.1815\n",
            "Epoch 88/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.9717\n",
            "Epoch 89/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 47.9429\n",
            "Epoch 90/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.0221\n",
            "Epoch 91/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 47.7532\n",
            "Epoch 92/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.6209\n",
            "Epoch 93/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.9859\n",
            "Epoch 94/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.4062\n",
            "Epoch 95/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 50.2560\n",
            "Epoch 96/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.9771\n",
            "Epoch 97/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 48.4195\n",
            "Epoch 98/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 49.3550\n",
            "Epoch 99/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 50.1117\n",
            "Epoch 100/100\n",
            "\u001b[1m1392/1392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 47.9063\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "Prediksi berat badan untuk tinggi 195 cm: 93.60 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datareg"
      ],
      "metadata": {
        "id": "UZaEFcNJtWL3",
        "outputId": "ad2e4973-fe05-4b82-92e9-5c350efdb316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0         player_name team_abbreviation   age  height  \\\n",
              "0              0       Dennis Rodman               CHI  36.0  198.12   \n",
              "1              1   Dwayne Schintzius               LAC  28.0  215.90   \n",
              "2              2        Earl Cureton               TOR  39.0  205.74   \n",
              "3              3         Ed O'Bannon               DAL  24.0  203.20   \n",
              "4              4         Ed Pinckney               MIA  34.0  205.74   \n",
              "...          ...                 ...               ...   ...     ...   \n",
              "11140      11140         Maxi Kleber               DAL  28.0  208.28   \n",
              "11141      11141  Melvin Frazier Jr.               ORL  23.0  195.58   \n",
              "11142      11142      Meyers Leonard               MIA  28.0  213.36   \n",
              "11143      11143        Norvel Pelle               PHI  27.0  208.28   \n",
              "11144      11144         Matt Thomas               TOR  25.0  193.04   \n",
              "\n",
              "           weight                      college  country draft_year  \\\n",
              "0       99.790240  Southeastern Oklahoma State      USA       1986   \n",
              "1      117.933920                      Florida      USA       1990   \n",
              "2       95.254320                Detroit Mercy      USA       1979   \n",
              "3      100.697424                         UCLA      USA       1995   \n",
              "4      108.862080                    Villanova      USA       1985   \n",
              "...           ...                          ...      ...        ...   \n",
              "11140  108.862080                          NaN  Germany  Undrafted   \n",
              "11141   97.522280                       Tulane      USA       2018   \n",
              "11142  117.933920                     Illinois      USA       2012   \n",
              "11143  104.779752                          NaN      USA  Undrafted   \n",
              "11144   86.182480                   Iowa State      USA  Undrafted   \n",
              "\n",
              "      draft_round  ...  pts   reb  ast  net_rating  oreb_pct  dreb_pct  \\\n",
              "0               2  ...  5.7  16.1  3.1        16.1     0.186     0.323   \n",
              "1               1  ...  2.3   1.5  0.3        12.3     0.078     0.151   \n",
              "2               3  ...  0.8   1.0  0.4        -2.1     0.105     0.102   \n",
              "3               1  ...  3.7   2.3  0.6        -8.7     0.060     0.149   \n",
              "4               1  ...  2.4   2.4  0.2       -11.2     0.109     0.179   \n",
              "...           ...  ...  ...   ...  ...         ...       ...       ...   \n",
              "11140   Undrafted  ...  9.1   5.4  1.1         4.6     0.056     0.140   \n",
              "11141           2  ...  1.2   0.3  0.1        -2.4     0.018     0.058   \n",
              "11142           1  ...  6.1   5.1  1.1         5.6     0.029     0.217   \n",
              "11143   Undrafted  ...  2.1   3.0  0.4       -16.4     0.085     0.237   \n",
              "11144   Undrafted  ...  4.5   1.4  0.5         1.0     0.017     0.104   \n",
              "\n",
              "       usg_pct  ts_pct  ast_pct   season  \n",
              "0        0.100   0.479    0.113  1996-97  \n",
              "1        0.175   0.430    0.048  1996-97  \n",
              "2        0.103   0.376    0.148  1996-97  \n",
              "3        0.167   0.399    0.077  1996-97  \n",
              "4        0.127   0.611    0.040  1996-97  \n",
              "...        ...     ...      ...      ...  \n",
              "11140    0.136   0.605    0.064  2019-20  \n",
              "11141    0.164   0.480    0.033  2019-20  \n",
              "11142    0.120   0.640    0.076  2019-20  \n",
              "11143    0.126   0.521    0.056  2019-20  \n",
              "11144    0.149   0.663    0.089  2019-20  \n",
              "\n",
              "[11136 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d84028d-b5a5-4a64-8625-c58ec207cd2f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>player_name</th>\n",
              "      <th>team_abbreviation</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>college</th>\n",
              "      <th>country</th>\n",
              "      <th>draft_year</th>\n",
              "      <th>draft_round</th>\n",
              "      <th>...</th>\n",
              "      <th>pts</th>\n",
              "      <th>reb</th>\n",
              "      <th>ast</th>\n",
              "      <th>net_rating</th>\n",
              "      <th>oreb_pct</th>\n",
              "      <th>dreb_pct</th>\n",
              "      <th>usg_pct</th>\n",
              "      <th>ts_pct</th>\n",
              "      <th>ast_pct</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Dennis Rodman</td>\n",
              "      <td>CHI</td>\n",
              "      <td>36.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>99.790240</td>\n",
              "      <td>Southeastern Oklahoma State</td>\n",
              "      <td>USA</td>\n",
              "      <td>1986</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>5.7</td>\n",
              "      <td>16.1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>16.1</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.113</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Dwayne Schintzius</td>\n",
              "      <td>LAC</td>\n",
              "      <td>28.0</td>\n",
              "      <td>215.90</td>\n",
              "      <td>117.933920</td>\n",
              "      <td>Florida</td>\n",
              "      <td>USA</td>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.048</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Earl Cureton</td>\n",
              "      <td>TOR</td>\n",
              "      <td>39.0</td>\n",
              "      <td>205.74</td>\n",
              "      <td>95.254320</td>\n",
              "      <td>Detroit Mercy</td>\n",
              "      <td>USA</td>\n",
              "      <td>1979</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.148</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ed O'Bannon</td>\n",
              "      <td>DAL</td>\n",
              "      <td>24.0</td>\n",
              "      <td>203.20</td>\n",
              "      <td>100.697424</td>\n",
              "      <td>UCLA</td>\n",
              "      <td>USA</td>\n",
              "      <td>1995</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3.7</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-8.7</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.399</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ed Pinckney</td>\n",
              "      <td>MIA</td>\n",
              "      <td>34.0</td>\n",
              "      <td>205.74</td>\n",
              "      <td>108.862080</td>\n",
              "      <td>Villanova</td>\n",
              "      <td>USA</td>\n",
              "      <td>1985</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-11.2</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11140</th>\n",
              "      <td>11140</td>\n",
              "      <td>Maxi Kleber</td>\n",
              "      <td>DAL</td>\n",
              "      <td>28.0</td>\n",
              "      <td>208.28</td>\n",
              "      <td>108.862080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>...</td>\n",
              "      <td>9.1</td>\n",
              "      <td>5.4</td>\n",
              "      <td>1.1</td>\n",
              "      <td>4.6</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.064</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11141</th>\n",
              "      <td>11141</td>\n",
              "      <td>Melvin Frazier Jr.</td>\n",
              "      <td>ORL</td>\n",
              "      <td>23.0</td>\n",
              "      <td>195.58</td>\n",
              "      <td>97.522280</td>\n",
              "      <td>Tulane</td>\n",
              "      <td>USA</td>\n",
              "      <td>2018</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.033</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11142</th>\n",
              "      <td>11142</td>\n",
              "      <td>Meyers Leonard</td>\n",
              "      <td>MIA</td>\n",
              "      <td>28.0</td>\n",
              "      <td>213.36</td>\n",
              "      <td>117.933920</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>USA</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>6.1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>5.6</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.640</td>\n",
              "      <td>0.076</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11143</th>\n",
              "      <td>11143</td>\n",
              "      <td>Norvel Pelle</td>\n",
              "      <td>PHI</td>\n",
              "      <td>27.0</td>\n",
              "      <td>208.28</td>\n",
              "      <td>104.779752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>USA</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>...</td>\n",
              "      <td>2.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-16.4</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.056</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11144</th>\n",
              "      <td>11144</td>\n",
              "      <td>Matt Thomas</td>\n",
              "      <td>TOR</td>\n",
              "      <td>25.0</td>\n",
              "      <td>193.04</td>\n",
              "      <td>86.182480</td>\n",
              "      <td>Iowa State</td>\n",
              "      <td>USA</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.089</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11136 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d84028d-b5a5-4a64-8625-c58ec207cd2f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d84028d-b5a5-4a64-8625-c58ec207cd2f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d84028d-b5a5-4a64-8625-c58ec207cd2f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-76a1f9e5-362c-4232-a608-4cce942ce392\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76a1f9e5-362c-4232-a608-4cce942ce392')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-76a1f9e5-362c-4232-a608-4cce942ce392 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_417064ca-9781-4594-afe0-955e375adc59\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('datareg')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_417064ca-9781-4594-afe0-955e375adc59 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('datareg');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "datareg"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = datareg.sort_values(by='height', ascending=True)\n",
        "df_sorted[(df_sorted['height']>= 190) & (df_sorted['height']<=200)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "EzbhHig0JP05",
        "outputId": "fc25fb7a-e635-461a-cc3f-fde5eb63b96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0       player_name team_abbreviation   age  height  \\\n",
              "10274      10274      Andre Ingram               LAL  33.0  190.50   \n",
              "11119      11119        Malik Monk               CHA  22.0  190.50   \n",
              "11120      11120      Malik Newman               CLE  23.0  190.50   \n",
              "11124      11124      Marcus Smart               BOS  26.0  190.50   \n",
              "9176        9176      Kyrie Irving               CLE  25.0  190.50   \n",
              "...          ...               ...               ...   ...     ...   \n",
              "10656      10656  Ignas Brazdeikis               NYK  21.0  198.12   \n",
              "10276      10276   Andrew Harrison               NOP  24.0  198.12   \n",
              "8419        8419          JR Smith               CLE  29.0  198.12   \n",
              "0              0     Dennis Rodman               CHI  36.0  198.12   \n",
              "6              6       Eddie Jones               LAL  25.0  198.12   \n",
              "\n",
              "           weight                      college    country draft_year  \\\n",
              "10274   86.182480          American University        USA  Undrafted   \n",
              "11119   90.718400                     Kentucky        USA       2017   \n",
              "11120   86.182480                       Kansas        USA  Undrafted   \n",
              "11124   99.790240               Oklahoma State        USA       2014   \n",
              "9176    87.543256                         Duke  Australia       2011   \n",
              "...           ...                          ...        ...        ...   \n",
              "10656  100.243832                     Michigan  Lithuania       2019   \n",
              "10276   96.615096                     Kentucky        USA       2015   \n",
              "8419   102.058200                          NaN        USA       2004   \n",
              "0       99.790240  Southeastern Oklahoma State        USA       1986   \n",
              "6       86.182480                       Temple        USA       1994   \n",
              "\n",
              "      draft_round  ...   pts   reb  ast  net_rating  oreb_pct  dreb_pct  \\\n",
              "10274   Undrafted  ...   0.0   0.5  0.0         9.6     0.059     0.053   \n",
              "11119           1  ...  10.3   2.9  2.1       -10.5     0.021     0.108   \n",
              "11120   Undrafted  ...   2.0   0.0  0.0       -30.4     0.000     0.000   \n",
              "11124           1  ...  13.3   3.7  4.9         7.0     0.021     0.085   \n",
              "9176            1  ...  25.2   3.2  5.8         5.1     0.023     0.075   \n",
              "...           ...  ...   ...   ...  ...         ...       ...       ...   \n",
              "10656           2  ...   1.9   0.6  0.4        -7.1     0.036     0.068   \n",
              "10276           2  ...   3.2   1.2  1.4        -5.5     0.032     0.074   \n",
              "8419            1  ...  12.1   3.1  2.8         2.8     0.017     0.102   \n",
              "0               2  ...   5.7  16.1  3.1        16.1     0.186     0.323   \n",
              "6               1  ...  17.2   4.1  3.4         4.1     0.035     0.091   \n",
              "\n",
              "       usg_pct  ts_pct  ast_pct   season  \n",
              "10274    0.184   0.000    0.000  2018-19  \n",
              "11119    0.220   0.530    0.168  2019-20  \n",
              "11120    0.571   0.347    0.000  2019-20  \n",
              "11124    0.186   0.519    0.215  2019-20  \n",
              "9176     0.302   0.580    0.278  2016-17  \n",
              "...        ...     ...      ...      ...  \n",
              "10656    0.218   0.351    0.114  2019-20  \n",
              "10276    0.171   0.443    0.194  2018-19  \n",
              "8419     0.191   0.538    0.145  2014-15  \n",
              "0        0.100   0.479    0.113  1996-97  \n",
              "6        0.209   0.559    0.149  1996-97  \n",
              "\n",
              "[3104 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12b21388-876a-4c6b-8dbf-0c9ea044a163\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>player_name</th>\n",
              "      <th>team_abbreviation</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>college</th>\n",
              "      <th>country</th>\n",
              "      <th>draft_year</th>\n",
              "      <th>draft_round</th>\n",
              "      <th>...</th>\n",
              "      <th>pts</th>\n",
              "      <th>reb</th>\n",
              "      <th>ast</th>\n",
              "      <th>net_rating</th>\n",
              "      <th>oreb_pct</th>\n",
              "      <th>dreb_pct</th>\n",
              "      <th>usg_pct</th>\n",
              "      <th>ts_pct</th>\n",
              "      <th>ast_pct</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10274</th>\n",
              "      <td>10274</td>\n",
              "      <td>Andre Ingram</td>\n",
              "      <td>LAL</td>\n",
              "      <td>33.0</td>\n",
              "      <td>190.50</td>\n",
              "      <td>86.182480</td>\n",
              "      <td>American University</td>\n",
              "      <td>USA</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2018-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11119</th>\n",
              "      <td>11119</td>\n",
              "      <td>Malik Monk</td>\n",
              "      <td>CHA</td>\n",
              "      <td>22.0</td>\n",
              "      <td>190.50</td>\n",
              "      <td>90.718400</td>\n",
              "      <td>Kentucky</td>\n",
              "      <td>USA</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>10.3</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.1</td>\n",
              "      <td>-10.5</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.168</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11120</th>\n",
              "      <td>11120</td>\n",
              "      <td>Malik Newman</td>\n",
              "      <td>CLE</td>\n",
              "      <td>23.0</td>\n",
              "      <td>190.50</td>\n",
              "      <td>86.182480</td>\n",
              "      <td>Kansas</td>\n",
              "      <td>USA</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>Undrafted</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-30.4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11124</th>\n",
              "      <td>11124</td>\n",
              "      <td>Marcus Smart</td>\n",
              "      <td>BOS</td>\n",
              "      <td>26.0</td>\n",
              "      <td>190.50</td>\n",
              "      <td>99.790240</td>\n",
              "      <td>Oklahoma State</td>\n",
              "      <td>USA</td>\n",
              "      <td>2014</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>13.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.215</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9176</th>\n",
              "      <td>9176</td>\n",
              "      <td>Kyrie Irving</td>\n",
              "      <td>CLE</td>\n",
              "      <td>25.0</td>\n",
              "      <td>190.50</td>\n",
              "      <td>87.543256</td>\n",
              "      <td>Duke</td>\n",
              "      <td>Australia</td>\n",
              "      <td>2011</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>25.2</td>\n",
              "      <td>3.2</td>\n",
              "      <td>5.8</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.278</td>\n",
              "      <td>2016-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10656</th>\n",
              "      <td>10656</td>\n",
              "      <td>Ignas Brazdeikis</td>\n",
              "      <td>NYK</td>\n",
              "      <td>21.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>100.243832</td>\n",
              "      <td>Michigan</td>\n",
              "      <td>Lithuania</td>\n",
              "      <td>2019</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-7.1</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.114</td>\n",
              "      <td>2019-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10276</th>\n",
              "      <td>10276</td>\n",
              "      <td>Andrew Harrison</td>\n",
              "      <td>NOP</td>\n",
              "      <td>24.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>96.615096</td>\n",
              "      <td>Kentucky</td>\n",
              "      <td>USA</td>\n",
              "      <td>2015</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.194</td>\n",
              "      <td>2018-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8419</th>\n",
              "      <td>8419</td>\n",
              "      <td>JR Smith</td>\n",
              "      <td>CLE</td>\n",
              "      <td>29.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>102.058200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>USA</td>\n",
              "      <td>2004</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>12.1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.145</td>\n",
              "      <td>2014-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Dennis Rodman</td>\n",
              "      <td>CHI</td>\n",
              "      <td>36.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>99.790240</td>\n",
              "      <td>Southeastern Oklahoma State</td>\n",
              "      <td>USA</td>\n",
              "      <td>1986</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>5.7</td>\n",
              "      <td>16.1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>16.1</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.113</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Eddie Jones</td>\n",
              "      <td>LAL</td>\n",
              "      <td>25.0</td>\n",
              "      <td>198.12</td>\n",
              "      <td>86.182480</td>\n",
              "      <td>Temple</td>\n",
              "      <td>USA</td>\n",
              "      <td>1994</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>17.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>4.1</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.559</td>\n",
              "      <td>0.149</td>\n",
              "      <td>1996-97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3104 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12b21388-876a-4c6b-8dbf-0c9ea044a163')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-12b21388-876a-4c6b-8dbf-0c9ea044a163 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-12b21388-876a-4c6b-8dbf-0c9ea044a163');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6894b7ed-61b6-4a81-b680-f8e586872362\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6894b7ed-61b6-4a81-b680-f8e586872362')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6894b7ed-61b6-4a81-b680-f8e586872362 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "datareg = pd.read_csv('all_seasons.csv', sep=',')\n",
        "\n",
        "datareg = datareg.dropna(subset=['height', 'weight'])\n",
        "x_train = datareg['height'].astype(float).values\n",
        "y_train = datareg['weight'].astype(float).values\n",
        "\n",
        "# dataset = tf.data.experimental.make_csv_dataset(\n",
        "#     file_pattern='all_seasons.csv',   # nama file CSV\n",
        "#     batch_size=8,                    # ukuran batch\n",
        "#     label_name='weight',             # kolom target (y)\n",
        "#     field_delim=';',                 # delimiter CSV\n",
        "#     num_epochs=1,                    # berapa kali data dibaca\n",
        "#     shuffle=True                     # acak data\n",
        "# )\n",
        "\n",
        "# Normalisasi\n",
        "x_mean, x_std = np.mean(x_train), np.std(x_train)\n",
        "y_mean, y_std = np.mean(y_train), np.std(y_train)\n",
        "x_train = (x_train - x_mean) / x_std\n",
        "y_train = (y_train - y_mean) / y_std\n",
        "\n",
        "# Ubah jadi dataset TensorFlow\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "# Shuffle dan batching\n",
        "dataset = dataset.shuffle(buffer_size=len(x_train)).batch(5).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(8, activation='relu', input_shape=[1]),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error') #setting ML\n",
        "\n",
        "# Callback early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model1.fit(dataset, epochs=100, verbose=1,  callbacks=[early_stop]) #training\n",
        "\n",
        "# Contoh prediksi: tinggi = 170 cm\n",
        "height_input = 195\n",
        "predicted_weight = model1.predict(np.array([[height_input]]))\n",
        "print(f\"Prediksi berat badan untuk tinggi {height_input} cm: {predicted_weight[0][0]:.2f} kg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5es-BboE4hI",
        "outputId": "2f1e6121-fc83-4266-b21b-60e65648addd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.4718\n",
            "Epoch 2/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3116\n",
            "Epoch 3/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3093\n",
            "Epoch 4/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3180\n",
            "Epoch 5/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2980\n",
            "Epoch 6/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3064\n",
            "Epoch 7/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3001\n",
            "Epoch 8/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3122\n",
            "Epoch 9/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3151\n",
            "Epoch 10/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3183\n",
            "Epoch 11/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3066\n",
            "Epoch 12/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3038\n",
            "Epoch 13/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3099\n",
            "Epoch 14/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.3088\n",
            "Epoch 15/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.3138\n",
            "Epoch 16/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3080\n",
            "Epoch 17/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.2978\n",
            "Epoch 18/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3073\n",
            "Epoch 19/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.3019\n",
            "Epoch 20/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3097\n",
            "Epoch 21/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3126\n",
            "Epoch 22/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3121\n",
            "Epoch 23/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3030\n",
            "Epoch 24/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.3119\n",
            "Epoch 25/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.3052\n",
            "Epoch 26/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3034\n",
            "Epoch 27/100\n",
            "\u001b[1m2228/2228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fdf383abf60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "Prediksi berat badan untuk tinggi 195 cm: 136.99 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Jenis Layer\n",
        "untuk data biasa\n",
        "```\n",
        "tf.keras.layers.Dense(64) #64 neuron\n",
        "```\n",
        "untuk data gambar\n",
        "```\n",
        "tf.keras.layers.Conv2D(32, (3, 3))  # 32 filters size 3x3\n",
        "```\n",
        "untuk data waktu (time series)\n",
        "```\n",
        "tf.keras.layers.LSTM(50)  # 50 LSTM units\n",
        "```\n",
        "\n",
        "# Jenis loss\n",
        "\n",
        "MSE - untuk regresi\n",
        "```\n",
        "loss='mean_squared_error'\n",
        "```\n",
        "klasifikasi binary\n",
        "```\n",
        "loss='binary_crossentropy'\n",
        "```\n",
        "klasifikasi multi-class\n",
        "```\n",
        "loss='categorical_crossentropy'\n",
        "```\n"
      ],
      "metadata": {
        "id": "w_g1o8bkFMxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Market Basket Analysis – Association Rules\n",
        "\n"
      ],
      "metadata": {
        "id": "opC4F3sN3Xs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketBasketModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, encoding_dim=6):\n",
        "        super(MarketBasketModel, self).__init__()\n",
        "\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(encoding_dim, activation='relu')\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "jxwzNmsMHDAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "data = pd.read_csv('dataset MBA.csv', sep=';')\n",
        "print(data, \"\\n\")\n",
        "\n",
        "# 2. Preprocess Data\n",
        "print(\"# Preprocessing data... \\n\")\n",
        "\n",
        "# Clean dan split Items List jadi list of items\n",
        "data['items'] = data['Items List'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
        "print(\"Data setelah cleaning:\")\n",
        "print(data[['ID', 'items']], \"\\n\")\n",
        "\n",
        "# 3. Buat mapping item -> index\n",
        "# Ambil semua item unik dari seluruh transaksi\n",
        "all_items = sorted(set(item for sublist in data['items'] for item in sublist))\n",
        "item_to_index = {item: i for i, item in enumerate(all_items)}\n",
        "index_to_item = {i: item for item, i in item_to_index.items()}\n",
        "n_items = len(all_items)\n",
        "\n",
        "print(f\"Total unique items: {n_items}\")\n",
        "print(\"Contoh mapping:\", list(item_to_index.items())[:5], \"\\n\")"
      ],
      "metadata": {
        "id": "rPx0lVXLzt_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311edcc0-83b4-4b56-cc44-bc68345bf86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID                                         Items List\n",
            "0   1                       Cookies, Egg, Milk, Sandwich\n",
            "1   2  Bottled Water, Burger, Chicken, Egg, Pizza, Salad\n",
            "2   3       Beacon, Bottled Water, Egg, Sandwich, Yogurt\n",
            "3   4                    Burger, Pie, Pizza, Salad, Soda\n",
            "4   5         Burger, Ice Cream, Pie, Pizza, Salad, Soda\n",
            "5   6      Chocolate Shake, Cookies, Egg, Milk, Sandwich\n",
            "6   7     Beacon, Chocolate Shake, Cookies, Milk, Yogurt\n",
            "7   8                              Beacon, Burger, Salad\n",
            "8   9               Cookies, Egg, Milk, Sandwich, Yogurt\n",
            "9  10      Chocolate Shake, Cookies, Egg, Milk, Sandwich \n",
            "\n",
            "# Preprocessing data... \n",
            "\n",
            "Data setelah cleaning:\n",
            "   ID                                              items\n",
            "0   1                     [Cookies, Egg, Milk, Sandwich]\n",
            "1   2  [Bottled Water, Burger, Chicken, Egg, Pizza, S...\n",
            "2   3     [Beacon, Bottled Water, Egg, Sandwich, Yogurt]\n",
            "3   4                  [Burger, Pie, Pizza, Salad, Soda]\n",
            "4   5       [Burger, Ice Cream, Pie, Pizza, Salad, Soda]\n",
            "5   6    [Chocolate Shake, Cookies, Egg, Milk, Sandwich]\n",
            "6   7   [Beacon, Chocolate Shake, Cookies, Milk, Yogurt]\n",
            "7   8                            [Beacon, Burger, Salad]\n",
            "8   9             [Cookies, Egg, Milk, Sandwich, Yogurt]\n",
            "9  10    [Chocolate Shake, Cookies, Egg, Milk, Sandwich] \n",
            "\n",
            "Total unique items: 15\n",
            "Contoh mapping: [('Beacon', 0), ('Bottled Water', 1), ('Burger', 2), ('Chicken', 3), ('Chocolate Shake', 4)] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_transaction(t):\n",
        "    vec = [0]*n_items\n",
        "    for item in t:\n",
        "        vec[item_to_index[item]] = 1\n",
        "    return vec"
      ],
      "metadata": {
        "id": "sjWwkBHgDNGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encode transaksi ke one-hot vector\n",
        "encoded = tf.constant([encode_transaction(t) for t in data['items']], dtype=tf.float32)\n",
        "print(\"Encoded:\")\n",
        "print(list(item_to_index))\n",
        "print(encoded)\n",
        "\n",
        "# 5. initialize model\n",
        "input_dim = n_items\n",
        "model = MarketBasketModel(input_dim, encoding_dim=6)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model Summary:\")\n",
        "model.build(input_shape=(None, input_dim))\n",
        "model.summary()\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "FIls-23JLv9U",
        "outputId": "2d8cb4af-a3cf-4b43-8d3e-a394a60d77a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded:\n",
            "['Beacon', 'Bottled Water', 'Burger', 'Chicken', 'Chocolate Shake', 'Cookies', 'Egg', 'Ice Cream', 'Milk', 'Pie', 'Pizza', 'Salad', 'Sandwich', 'Soda', 'Yogurt']\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.]], shape=(10, 15), dtype=float32)\n",
            "Model Summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'market_basket_model_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"market_basket_model_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"market_basket_model_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_17 (\u001b[38;5;33mSequential\u001b[0m)      │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_18 (\u001b[38;5;33mSequential\u001b[0m)      │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training model\n",
        "model.fit(encoded, encoded, epochs=20, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdi3sJ2HMv-n",
        "outputId": "5e5bb993-d8ba-43ab-8158-9c91f97e8edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.6926\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: 0.6903\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.6892\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.6885\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.6869\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.6852\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.6832\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.6833\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.6821\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.6800\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.6787\n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.6758\n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.6742\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2167 - loss: 0.6740\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2167 - loss: 0.6714\n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1750 - loss: 0.6715\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1750 - loss: 0.6707\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2167 - loss: 0.6664\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1750 - loss: 0.6666\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1750 - loss: 0.6643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b9597858e30>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_products(input_items, top_k=3):\n",
        "    # Create input vector\n",
        "    input_vector = np.array([[1 if product in input_items else 0 for product in all_items]])\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(input_vector, verbose=0)[0]\n",
        "\n",
        "    # Filter products yang belum dipilih\n",
        "    available_products = [p for p in all_items if p not in input_items]\n",
        "    available_indices = [i for i, p in enumerate(all_items) if p not in input_items]\n",
        "\n",
        "    # Get top recommendations\n",
        "    top_indices = np.argsort(predictions[available_indices])[-top_k:][::-1]\n",
        "    top_products = [available_products[i] for i in top_indices]\n",
        "    top_scores = [float(predictions[available_indices[i]]) for i in top_indices]\n",
        "    top_scores = [round(score, 2) for score in top_scores]\n",
        "\n",
        "    return list(zip(top_products, top_scores))"
      ],
      "metadata": {
        "id": "FcpJ6zSPN9sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tes rekomendasi untuk satu transaksi parsial\n",
        "test_vector = encoded[0:1]  # ambil transaksi pertama\n",
        "pred = model.predict(test_vector)\n",
        "\n",
        "# Urutkan item dengan probabilitas tertinggi\n",
        "sorted_idx = np.argsort(pred[0])[::-1]\n",
        "print(\"Rekomendasi top item:\")\n",
        "for idx in sorted_idx[:5]:\n",
        "    print(f\"- {index_to_item[idx]} ({pred[0][idx]:.2f})\")\n",
        "print()\n",
        "\n",
        "contoh_input = ['Yogurt']   # ubah sesuai produk dataset\n",
        "print(f\"Rekomendasi untuk {contoh_input}:\")\n",
        "print(recommend_products(contoh_input, top_k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bPfWrGlNDv8",
        "outputId": "a8c067a9-efb1-4365-89a6-c17a649ddf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Rekomendasi top item:\n",
            "- Yogurt (0.55)\n",
            "- Egg (0.54)\n",
            "- Burger (0.53)\n",
            "- Cookies (0.53)\n",
            "- Bottled Water (0.53)\n",
            "\n",
            "Rekomendasi untuk ['Yogurt']:\n",
            "[('Bottled Water', 0.52), ('Cookies', 0.52), ('Burger', 0.51)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Health Categories - Classification"
      ],
      "metadata": {
        "id": "SrM9RHju8ASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengaturan\n",
        "DATA_URL = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "COLUMN_NAMES = [\n",
        "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
        "    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'\n",
        "]\n",
        "BATCH_SIZE = 32 # jumlah data yg diproses sekaligus dalam satu langkah penelitian\n",
        "EPOCHS = 200 # berlatih mengulang data sebanyak 200 kali\n",
        "SEED = 42 # setiap kode dijalankan, hasil acak\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n"
      ],
      "metadata": {
        "id": "nyG1dC4h-m_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Download and load CSV\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(DATA_URL, names=COLUMN_NAMES, header=None)\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "# Menampilkan semua baris (tanpa batas)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(df, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wdPjvBe8GyU",
        "outputId": "4e72df97-5224-4727-d31c-d4a21383482e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Loaded shape: (768, 9)\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "5              5      116             74              0        0  25.6   \n",
            "6              3       78             50             32       88  31.0   \n",
            "7             10      115              0              0        0  35.3   \n",
            "8              2      197             70             45      543  30.5   \n",
            "9              8      125             96              0        0   0.0   \n",
            "10             4      110             92              0        0  37.6   \n",
            "11            10      168             74              0        0  38.0   \n",
            "12            10      139             80              0        0  27.1   \n",
            "13             1      189             60             23      846  30.1   \n",
            "14             5      166             72             19      175  25.8   \n",
            "15             7      100              0              0        0  30.0   \n",
            "16             0      118             84             47      230  45.8   \n",
            "17             7      107             74              0        0  29.6   \n",
            "18             1      103             30             38       83  43.3   \n",
            "19             1      115             70             30       96  34.6   \n",
            "20             3      126             88             41      235  39.3   \n",
            "21             8       99             84              0        0  35.4   \n",
            "22             7      196             90              0        0  39.8   \n",
            "23             9      119             80             35        0  29.0   \n",
            "24            11      143             94             33      146  36.6   \n",
            "25            10      125             70             26      115  31.1   \n",
            "26             7      147             76              0        0  39.4   \n",
            "27             1       97             66             15      140  23.2   \n",
            "28            13      145             82             19      110  22.2   \n",
            "29             5      117             92              0        0  34.1   \n",
            "30             5      109             75             26        0  36.0   \n",
            "31             3      158             76             36      245  31.6   \n",
            "32             3       88             58             11       54  24.8   \n",
            "33             6       92             92              0        0  19.9   \n",
            "34            10      122             78             31        0  27.6   \n",
            "35             4      103             60             33      192  24.0   \n",
            "36            11      138             76              0        0  33.2   \n",
            "37             9      102             76             37        0  32.9   \n",
            "38             2       90             68             42        0  38.2   \n",
            "39             4      111             72             47      207  37.1   \n",
            "40             3      180             64             25       70  34.0   \n",
            "41             7      133             84              0        0  40.2   \n",
            "42             7      106             92             18        0  22.7   \n",
            "43             9      171            110             24      240  45.4   \n",
            "44             7      159             64              0        0  27.4   \n",
            "45             0      180             66             39        0  42.0   \n",
            "46             1      146             56              0        0  29.7   \n",
            "47             2       71             70             27        0  28.0   \n",
            "48             7      103             66             32        0  39.1   \n",
            "49             7      105              0              0        0   0.0   \n",
            "50             1      103             80             11       82  19.4   \n",
            "51             1      101             50             15       36  24.2   \n",
            "52             5       88             66             21       23  24.4   \n",
            "53             8      176             90             34      300  33.7   \n",
            "54             7      150             66             42      342  34.7   \n",
            "55             1       73             50             10        0  23.0   \n",
            "56             7      187             68             39      304  37.7   \n",
            "57             0      100             88             60      110  46.8   \n",
            "58             0      146             82              0        0  40.5   \n",
            "59             0      105             64             41      142  41.5   \n",
            "60             2       84              0              0        0   0.0   \n",
            "61             8      133             72              0        0  32.9   \n",
            "62             5       44             62              0        0  25.0   \n",
            "63             2      141             58             34      128  25.4   \n",
            "64             7      114             66              0        0  32.8   \n",
            "65             5       99             74             27        0  29.0   \n",
            "66             0      109             88             30        0  32.5   \n",
            "67             2      109             92              0        0  42.7   \n",
            "68             1       95             66             13       38  19.6   \n",
            "69             4      146             85             27      100  28.9   \n",
            "70             2      100             66             20       90  32.9   \n",
            "71             5      139             64             35      140  28.6   \n",
            "72            13      126             90              0        0  43.4   \n",
            "73             4      129             86             20      270  35.1   \n",
            "74             1       79             75             30        0  32.0   \n",
            "75             1        0             48             20        0  24.7   \n",
            "76             7       62             78              0        0  32.6   \n",
            "77             5       95             72             33        0  37.7   \n",
            "78             0      131              0              0        0  43.2   \n",
            "79             2      112             66             22        0  25.0   \n",
            "80             3      113             44             13        0  22.4   \n",
            "81             2       74              0              0        0   0.0   \n",
            "82             7       83             78             26       71  29.3   \n",
            "83             0      101             65             28        0  24.6   \n",
            "84             5      137            108              0        0  48.8   \n",
            "85             2      110             74             29      125  32.4   \n",
            "86            13      106             72             54        0  36.6   \n",
            "87             2      100             68             25       71  38.5   \n",
            "88            15      136             70             32      110  37.1   \n",
            "89             1      107             68             19        0  26.5   \n",
            "90             1       80             55              0        0  19.1   \n",
            "91             4      123             80             15      176  32.0   \n",
            "92             7       81             78             40       48  46.7   \n",
            "93             4      134             72              0        0  23.8   \n",
            "94             2      142             82             18       64  24.7   \n",
            "95             6      144             72             27      228  33.9   \n",
            "96             2       92             62             28        0  31.6   \n",
            "97             1       71             48             18       76  20.4   \n",
            "98             6       93             50             30       64  28.7   \n",
            "99             1      122             90             51      220  49.7   \n",
            "100            1      163             72              0        0  39.0   \n",
            "101            1      151             60              0        0  26.1   \n",
            "102            0      125             96              0        0  22.5   \n",
            "103            1       81             72             18       40  26.6   \n",
            "104            2       85             65              0        0  39.6   \n",
            "105            1      126             56             29      152  28.7   \n",
            "106            1       96            122              0        0  22.4   \n",
            "107            4      144             58             28      140  29.5   \n",
            "108            3       83             58             31       18  34.3   \n",
            "109            0       95             85             25       36  37.4   \n",
            "110            3      171             72             33      135  33.3   \n",
            "111            8      155             62             26      495  34.0   \n",
            "112            1       89             76             34       37  31.2   \n",
            "113            4       76             62              0        0  34.0   \n",
            "114            7      160             54             32      175  30.5   \n",
            "115            4      146             92              0        0  31.2   \n",
            "116            5      124             74              0        0  34.0   \n",
            "117            5       78             48              0        0  33.7   \n",
            "118            4       97             60             23        0  28.2   \n",
            "119            4       99             76             15       51  23.2   \n",
            "120            0      162             76             56      100  53.2   \n",
            "121            6      111             64             39        0  34.2   \n",
            "122            2      107             74             30      100  33.6   \n",
            "123            5      132             80              0        0  26.8   \n",
            "124            0      113             76              0        0  33.3   \n",
            "125            1       88             30             42       99  55.0   \n",
            "126            3      120             70             30      135  42.9   \n",
            "127            1      118             58             36       94  33.3   \n",
            "128            1      117             88             24      145  34.5   \n",
            "129            0      105             84              0        0  27.9   \n",
            "130            4      173             70             14      168  29.7   \n",
            "131            9      122             56              0        0  33.3   \n",
            "132            3      170             64             37      225  34.5   \n",
            "133            8       84             74             31        0  38.3   \n",
            "134            2       96             68             13       49  21.1   \n",
            "135            2      125             60             20      140  33.8   \n",
            "136            0      100             70             26       50  30.8   \n",
            "137            0       93             60             25       92  28.7   \n",
            "138            0      129             80              0        0  31.2   \n",
            "139            5      105             72             29      325  36.9   \n",
            "140            3      128             78              0        0  21.1   \n",
            "141            5      106             82             30        0  39.5   \n",
            "142            2      108             52             26       63  32.5   \n",
            "143           10      108             66              0        0  32.4   \n",
            "144            4      154             62             31      284  32.8   \n",
            "145            0      102             75             23        0   0.0   \n",
            "146            9       57             80             37        0  32.8   \n",
            "147            2      106             64             35      119  30.5   \n",
            "148            5      147             78              0        0  33.7   \n",
            "149            2       90             70             17        0  27.3   \n",
            "150            1      136             74             50      204  37.4   \n",
            "151            4      114             65              0        0  21.9   \n",
            "152            9      156             86             28      155  34.3   \n",
            "153            1      153             82             42      485  40.6   \n",
            "154            8      188             78              0        0  47.9   \n",
            "155            7      152             88             44        0  50.0   \n",
            "156            2       99             52             15       94  24.6   \n",
            "157            1      109             56             21      135  25.2   \n",
            "158            2       88             74             19       53  29.0   \n",
            "159           17      163             72             41      114  40.9   \n",
            "160            4      151             90             38        0  29.7   \n",
            "161            7      102             74             40      105  37.2   \n",
            "162            0      114             80             34      285  44.2   \n",
            "163            2      100             64             23        0  29.7   \n",
            "164            0      131             88              0        0  31.6   \n",
            "165            6      104             74             18      156  29.9   \n",
            "166            3      148             66             25        0  32.5   \n",
            "167            4      120             68              0        0  29.6   \n",
            "168            4      110             66              0        0  31.9   \n",
            "169            3      111             90             12       78  28.4   \n",
            "170            6      102             82              0        0  30.8   \n",
            "171            6      134             70             23      130  35.4   \n",
            "172            2       87              0             23        0  28.9   \n",
            "173            1       79             60             42       48  43.5   \n",
            "174            2       75             64             24       55  29.7   \n",
            "175            8      179             72             42      130  32.7   \n",
            "176            6       85             78              0        0  31.2   \n",
            "177            0      129            110             46      130  67.1   \n",
            "178            5      143             78              0        0  45.0   \n",
            "179            5      130             82              0        0  39.1   \n",
            "180            6       87             80              0        0  23.2   \n",
            "181            0      119             64             18       92  34.9   \n",
            "182            1        0             74             20       23  27.7   \n",
            "183            5       73             60              0        0  26.8   \n",
            "184            4      141             74              0        0  27.6   \n",
            "185            7      194             68             28        0  35.9   \n",
            "186            8      181             68             36      495  30.1   \n",
            "187            1      128             98             41       58  32.0   \n",
            "188            8      109             76             39      114  27.9   \n",
            "189            5      139             80             35      160  31.6   \n",
            "190            3      111             62              0        0  22.6   \n",
            "191            9      123             70             44       94  33.1   \n",
            "192            7      159             66              0        0  30.4   \n",
            "193           11      135              0              0        0  52.3   \n",
            "194            8       85             55             20        0  24.4   \n",
            "195            5      158             84             41      210  39.4   \n",
            "196            1      105             58              0        0  24.3   \n",
            "197            3      107             62             13       48  22.9   \n",
            "198            4      109             64             44       99  34.8   \n",
            "199            4      148             60             27      318  30.9   \n",
            "200            0      113             80             16        0  31.0   \n",
            "201            1      138             82              0        0  40.1   \n",
            "202            0      108             68             20        0  27.3   \n",
            "203            2       99             70             16       44  20.4   \n",
            "204            6      103             72             32      190  37.7   \n",
            "205            5      111             72             28        0  23.9   \n",
            "206            8      196             76             29      280  37.5   \n",
            "207            5      162            104              0        0  37.7   \n",
            "208            1       96             64             27       87  33.2   \n",
            "209            7      184             84             33        0  35.5   \n",
            "210            2       81             60             22        0  27.7   \n",
            "211            0      147             85             54        0  42.8   \n",
            "212            7      179             95             31        0  34.2   \n",
            "213            0      140             65             26      130  42.6   \n",
            "214            9      112             82             32      175  34.2   \n",
            "215           12      151             70             40      271  41.8   \n",
            "216            5      109             62             41      129  35.8   \n",
            "217            6      125             68             30      120  30.0   \n",
            "218            5       85             74             22        0  29.0   \n",
            "219            5      112             66              0        0  37.8   \n",
            "220            0      177             60             29      478  34.6   \n",
            "221            2      158             90              0        0  31.6   \n",
            "222            7      119              0              0        0  25.2   \n",
            "223            7      142             60             33      190  28.8   \n",
            "224            1      100             66             15       56  23.6   \n",
            "225            1       87             78             27       32  34.6   \n",
            "226            0      101             76              0        0  35.7   \n",
            "227            3      162             52             38        0  37.2   \n",
            "228            4      197             70             39      744  36.7   \n",
            "229            0      117             80             31       53  45.2   \n",
            "230            4      142             86              0        0  44.0   \n",
            "231            6      134             80             37      370  46.2   \n",
            "232            1       79             80             25       37  25.4   \n",
            "233            4      122             68              0        0  35.0   \n",
            "234            3       74             68             28       45  29.7   \n",
            "235            4      171             72              0        0  43.6   \n",
            "236            7      181             84             21      192  35.9   \n",
            "237            0      179             90             27        0  44.1   \n",
            "238            9      164             84             21        0  30.8   \n",
            "239            0      104             76              0        0  18.4   \n",
            "240            1       91             64             24        0  29.2   \n",
            "241            4       91             70             32       88  33.1   \n",
            "242            3      139             54              0        0  25.6   \n",
            "243            6      119             50             22      176  27.1   \n",
            "244            2      146             76             35      194  38.2   \n",
            "245            9      184             85             15        0  30.0   \n",
            "246           10      122             68              0        0  31.2   \n",
            "247            0      165             90             33      680  52.3   \n",
            "248            9      124             70             33      402  35.4   \n",
            "249            1      111             86             19        0  30.1   \n",
            "250            9      106             52              0        0  31.2   \n",
            "251            2      129             84              0        0  28.0   \n",
            "252            2       90             80             14       55  24.4   \n",
            "253            0       86             68             32        0  35.8   \n",
            "254           12       92             62              7      258  27.6   \n",
            "255            1      113             64             35        0  33.6   \n",
            "256            3      111             56             39        0  30.1   \n",
            "257            2      114             68             22        0  28.7   \n",
            "258            1      193             50             16      375  25.9   \n",
            "259           11      155             76             28      150  33.3   \n",
            "260            3      191             68             15      130  30.9   \n",
            "261            3      141              0              0        0  30.0   \n",
            "262            4       95             70             32        0  32.1   \n",
            "263            3      142             80             15        0  32.4   \n",
            "264            4      123             62              0        0  32.0   \n",
            "265            5       96             74             18       67  33.6   \n",
            "266            0      138              0              0        0  36.3   \n",
            "267            2      128             64             42        0  40.0   \n",
            "268            0      102             52              0        0  25.1   \n",
            "269            2      146              0              0        0  27.5   \n",
            "270           10      101             86             37        0  45.6   \n",
            "271            2      108             62             32       56  25.2   \n",
            "272            3      122             78              0        0  23.0   \n",
            "273            1       71             78             50       45  33.2   \n",
            "274           13      106             70              0        0  34.2   \n",
            "275            2      100             70             52       57  40.5   \n",
            "276            7      106             60             24        0  26.5   \n",
            "277            0      104             64             23      116  27.8   \n",
            "278            5      114             74              0        0  24.9   \n",
            "279            2      108             62             10      278  25.3   \n",
            "280            0      146             70              0        0  37.9   \n",
            "281           10      129             76             28      122  35.9   \n",
            "282            7      133             88             15      155  32.4   \n",
            "283            7      161             86              0        0  30.4   \n",
            "284            2      108             80              0        0  27.0   \n",
            "285            7      136             74             26      135  26.0   \n",
            "286            5      155             84             44      545  38.7   \n",
            "287            1      119             86             39      220  45.6   \n",
            "288            4       96             56             17       49  20.8   \n",
            "289            5      108             72             43       75  36.1   \n",
            "290            0       78             88             29       40  36.9   \n",
            "291            0      107             62             30       74  36.6   \n",
            "292            2      128             78             37      182  43.3   \n",
            "293            1      128             48             45      194  40.5   \n",
            "294            0      161             50              0        0  21.9   \n",
            "295            6      151             62             31      120  35.5   \n",
            "296            2      146             70             38      360  28.0   \n",
            "297            0      126             84             29      215  30.7   \n",
            "298           14      100             78             25      184  36.6   \n",
            "299            8      112             72              0        0  23.6   \n",
            "300            0      167              0              0        0  32.3   \n",
            "301            2      144             58             33      135  31.6   \n",
            "302            5       77             82             41       42  35.8   \n",
            "303            5      115             98              0        0  52.9   \n",
            "304            3      150             76              0        0  21.0   \n",
            "305            2      120             76             37      105  39.7   \n",
            "306           10      161             68             23      132  25.5   \n",
            "307            0      137             68             14      148  24.8   \n",
            "308            0      128             68             19      180  30.5   \n",
            "309            2      124             68             28      205  32.9   \n",
            "310            6       80             66             30        0  26.2   \n",
            "311            0      106             70             37      148  39.4   \n",
            "312            2      155             74             17       96  26.6   \n",
            "313            3      113             50             10       85  29.5   \n",
            "314            7      109             80             31        0  35.9   \n",
            "315            2      112             68             22       94  34.1   \n",
            "316            3       99             80             11       64  19.3   \n",
            "317            3      182             74              0        0  30.5   \n",
            "318            3      115             66             39      140  38.1   \n",
            "319            6      194             78              0        0  23.5   \n",
            "320            4      129             60             12      231  27.5   \n",
            "321            3      112             74             30        0  31.6   \n",
            "322            0      124             70             20        0  27.4   \n",
            "323           13      152             90             33       29  26.8   \n",
            "324            2      112             75             32        0  35.7   \n",
            "325            1      157             72             21      168  25.6   \n",
            "326            1      122             64             32      156  35.1   \n",
            "327           10      179             70              0        0  35.1   \n",
            "328            2      102             86             36      120  45.5   \n",
            "329            6      105             70             32       68  30.8   \n",
            "330            8      118             72             19        0  23.1   \n",
            "331            2       87             58             16       52  32.7   \n",
            "332            1      180              0              0        0  43.3   \n",
            "333           12      106             80              0        0  23.6   \n",
            "334            1       95             60             18       58  23.9   \n",
            "335            0      165             76             43      255  47.9   \n",
            "336            0      117              0              0        0  33.8   \n",
            "337            5      115             76              0        0  31.2   \n",
            "338            9      152             78             34      171  34.2   \n",
            "339            7      178             84              0        0  39.9   \n",
            "340            1      130             70             13      105  25.9   \n",
            "341            1       95             74             21       73  25.9   \n",
            "342            1        0             68             35        0  32.0   \n",
            "343            5      122             86              0        0  34.7   \n",
            "344            8       95             72              0        0  36.8   \n",
            "345            8      126             88             36      108  38.5   \n",
            "346            1      139             46             19       83  28.7   \n",
            "347            3      116              0              0        0  23.5   \n",
            "348            3       99             62             19       74  21.8   \n",
            "349            5        0             80             32        0  41.0   \n",
            "350            4       92             80              0        0  42.2   \n",
            "351            4      137             84              0        0  31.2   \n",
            "352            3       61             82             28        0  34.4   \n",
            "353            1       90             62             12       43  27.2   \n",
            "354            3       90             78              0        0  42.7   \n",
            "355            9      165             88              0        0  30.4   \n",
            "356            1      125             50             40      167  33.3   \n",
            "357           13      129              0             30        0  39.9   \n",
            "358           12       88             74             40       54  35.3   \n",
            "359            1      196             76             36      249  36.5   \n",
            "360            5      189             64             33      325  31.2   \n",
            "361            5      158             70              0        0  29.8   \n",
            "362            5      103            108             37        0  39.2   \n",
            "363            4      146             78              0        0  38.5   \n",
            "364            4      147             74             25      293  34.9   \n",
            "365            5       99             54             28       83  34.0   \n",
            "366            6      124             72              0        0  27.6   \n",
            "367            0      101             64             17        0  21.0   \n",
            "368            3       81             86             16       66  27.5   \n",
            "369            1      133            102             28      140  32.8   \n",
            "370            3      173             82             48      465  38.4   \n",
            "371            0      118             64             23       89   0.0   \n",
            "372            0       84             64             22       66  35.8   \n",
            "373            2      105             58             40       94  34.9   \n",
            "374            2      122             52             43      158  36.2   \n",
            "375           12      140             82             43      325  39.2   \n",
            "376            0       98             82             15       84  25.2   \n",
            "377            1       87             60             37       75  37.2   \n",
            "378            4      156             75              0        0  48.3   \n",
            "379            0       93            100             39       72  43.4   \n",
            "380            1      107             72             30       82  30.8   \n",
            "381            0      105             68             22        0  20.0   \n",
            "382            1      109             60              8      182  25.4   \n",
            "383            1       90             62             18       59  25.1   \n",
            "384            1      125             70             24      110  24.3   \n",
            "385            1      119             54             13       50  22.3   \n",
            "386            5      116             74             29        0  32.3   \n",
            "387            8      105            100             36        0  43.3   \n",
            "388            5      144             82             26      285  32.0   \n",
            "389            3      100             68             23       81  31.6   \n",
            "390            1      100             66             29      196  32.0   \n",
            "391            5      166             76              0        0  45.7   \n",
            "392            1      131             64             14      415  23.7   \n",
            "393            4      116             72             12       87  22.1   \n",
            "394            4      158             78              0        0  32.9   \n",
            "395            2      127             58             24      275  27.7   \n",
            "396            3       96             56             34      115  24.7   \n",
            "397            0      131             66             40        0  34.3   \n",
            "398            3       82             70              0        0  21.1   \n",
            "399            3      193             70             31        0  34.9   \n",
            "400            4       95             64              0        0  32.0   \n",
            "401            6      137             61              0        0  24.2   \n",
            "402            5      136             84             41       88  35.0   \n",
            "403            9       72             78             25        0  31.6   \n",
            "404            5      168             64              0        0  32.9   \n",
            "405            2      123             48             32      165  42.1   \n",
            "406            4      115             72              0        0  28.9   \n",
            "407            0      101             62              0        0  21.9   \n",
            "408            8      197             74              0        0  25.9   \n",
            "409            1      172             68             49      579  42.4   \n",
            "410            6      102             90             39        0  35.7   \n",
            "411            1      112             72             30      176  34.4   \n",
            "412            1      143             84             23      310  42.4   \n",
            "413            1      143             74             22       61  26.2   \n",
            "414            0      138             60             35      167  34.6   \n",
            "415            3      173             84             33      474  35.7   \n",
            "416            1       97             68             21        0  27.2   \n",
            "417            4      144             82             32        0  38.5   \n",
            "418            1       83             68              0        0  18.2   \n",
            "419            3      129             64             29      115  26.4   \n",
            "420            1      119             88             41      170  45.3   \n",
            "421            2       94             68             18       76  26.0   \n",
            "422            0      102             64             46       78  40.6   \n",
            "423            2      115             64             22        0  30.8   \n",
            "424            8      151             78             32      210  42.9   \n",
            "425            4      184             78             39      277  37.0   \n",
            "426            0       94              0              0        0   0.0   \n",
            "427            1      181             64             30      180  34.1   \n",
            "428            0      135             94             46      145  40.6   \n",
            "429            1       95             82             25      180  35.0   \n",
            "430            2       99              0              0        0  22.2   \n",
            "431            3       89             74             16       85  30.4   \n",
            "432            1       80             74             11       60  30.0   \n",
            "433            2      139             75              0        0  25.6   \n",
            "434            1       90             68              8        0  24.5   \n",
            "435            0      141              0              0        0  42.4   \n",
            "436           12      140             85             33        0  37.4   \n",
            "437            5      147             75              0        0  29.9   \n",
            "438            1       97             70             15        0  18.2   \n",
            "439            6      107             88              0        0  36.8   \n",
            "440            0      189            104             25        0  34.3   \n",
            "441            2       83             66             23       50  32.2   \n",
            "442            4      117             64             27      120  33.2   \n",
            "443            8      108             70              0        0  30.5   \n",
            "444            4      117             62             12        0  29.7   \n",
            "445            0      180             78             63       14  59.4   \n",
            "446            1      100             72             12       70  25.3   \n",
            "447            0       95             80             45       92  36.5   \n",
            "448            0      104             64             37       64  33.6   \n",
            "449            0      120             74             18       63  30.5   \n",
            "450            1       82             64             13       95  21.2   \n",
            "451            2      134             70              0        0  28.9   \n",
            "452            0       91             68             32      210  39.9   \n",
            "453            2      119              0              0        0  19.6   \n",
            "454            2      100             54             28      105  37.8   \n",
            "455           14      175             62             30        0  33.6   \n",
            "456            1      135             54              0        0  26.7   \n",
            "457            5       86             68             28       71  30.2   \n",
            "458           10      148             84             48      237  37.6   \n",
            "459            9      134             74             33       60  25.9   \n",
            "460            9      120             72             22       56  20.8   \n",
            "461            1       71             62              0        0  21.8   \n",
            "462            8       74             70             40       49  35.3   \n",
            "463            5       88             78             30        0  27.6   \n",
            "464           10      115             98              0        0  24.0   \n",
            "465            0      124             56             13      105  21.8   \n",
            "466            0       74             52             10       36  27.8   \n",
            "467            0       97             64             36      100  36.8   \n",
            "468            8      120              0              0        0  30.0   \n",
            "469            6      154             78             41      140  46.1   \n",
            "470            1      144             82             40        0  41.3   \n",
            "471            0      137             70             38        0  33.2   \n",
            "472            0      119             66             27        0  38.8   \n",
            "473            7      136             90              0        0  29.9   \n",
            "474            4      114             64              0        0  28.9   \n",
            "475            0      137             84             27        0  27.3   \n",
            "476            2      105             80             45      191  33.7   \n",
            "477            7      114             76             17      110  23.8   \n",
            "478            8      126             74             38       75  25.9   \n",
            "479            4      132             86             31        0  28.0   \n",
            "480            3      158             70             30      328  35.5   \n",
            "481            0      123             88             37        0  35.2   \n",
            "482            4       85             58             22       49  27.8   \n",
            "483            0       84             82             31      125  38.2   \n",
            "484            0      145              0              0        0  44.2   \n",
            "485            0      135             68             42      250  42.3   \n",
            "486            1      139             62             41      480  40.7   \n",
            "487            0      173             78             32      265  46.5   \n",
            "488            4       99             72             17        0  25.6   \n",
            "489            8      194             80              0        0  26.1   \n",
            "490            2       83             65             28       66  36.8   \n",
            "491            2       89             90             30        0  33.5   \n",
            "492            4       99             68             38        0  32.8   \n",
            "493            4      125             70             18      122  28.9   \n",
            "494            3       80              0              0        0   0.0   \n",
            "495            6      166             74              0        0  26.6   \n",
            "496            5      110             68              0        0  26.0   \n",
            "497            2       81             72             15       76  30.1   \n",
            "498            7      195             70             33      145  25.1   \n",
            "499            6      154             74             32      193  29.3   \n",
            "500            2      117             90             19       71  25.2   \n",
            "501            3       84             72             32        0  37.2   \n",
            "502            6        0             68             41        0  39.0   \n",
            "503            7       94             64             25       79  33.3   \n",
            "504            3       96             78             39        0  37.3   \n",
            "505           10       75             82              0        0  33.3   \n",
            "506            0      180             90             26       90  36.5   \n",
            "507            1      130             60             23      170  28.6   \n",
            "508            2       84             50             23       76  30.4   \n",
            "509            8      120             78              0        0  25.0   \n",
            "510           12       84             72             31        0  29.7   \n",
            "511            0      139             62             17      210  22.1   \n",
            "512            9       91             68              0        0  24.2   \n",
            "513            2       91             62              0        0  27.3   \n",
            "514            3       99             54             19       86  25.6   \n",
            "515            3      163             70             18      105  31.6   \n",
            "516            9      145             88             34      165  30.3   \n",
            "517            7      125             86              0        0  37.6   \n",
            "518           13       76             60              0        0  32.8   \n",
            "519            6      129             90              7      326  19.6   \n",
            "520            2       68             70             32       66  25.0   \n",
            "521            3      124             80             33      130  33.2   \n",
            "522            6      114              0              0        0   0.0   \n",
            "523            9      130             70              0        0  34.2   \n",
            "524            3      125             58              0        0  31.6   \n",
            "525            3       87             60             18        0  21.8   \n",
            "526            1       97             64             19       82  18.2   \n",
            "527            3      116             74             15      105  26.3   \n",
            "528            0      117             66             31      188  30.8   \n",
            "529            0      111             65              0        0  24.6   \n",
            "530            2      122             60             18      106  29.8   \n",
            "531            0      107             76              0        0  45.3   \n",
            "532            1       86             66             52       65  41.3   \n",
            "533            6       91              0              0        0  29.8   \n",
            "534            1       77             56             30       56  33.3   \n",
            "535            4      132              0              0        0  32.9   \n",
            "536            0      105             90              0        0  29.6   \n",
            "537            0       57             60              0        0  21.7   \n",
            "538            0      127             80             37      210  36.3   \n",
            "539            3      129             92             49      155  36.4   \n",
            "540            8      100             74             40      215  39.4   \n",
            "541            3      128             72             25      190  32.4   \n",
            "542           10       90             85             32        0  34.9   \n",
            "543            4       84             90             23       56  39.5   \n",
            "544            1       88             78             29       76  32.0   \n",
            "545            8      186             90             35      225  34.5   \n",
            "546            5      187             76             27      207  43.6   \n",
            "547            4      131             68             21      166  33.1   \n",
            "548            1      164             82             43       67  32.8   \n",
            "549            4      189            110             31        0  28.5   \n",
            "550            1      116             70             28        0  27.4   \n",
            "551            3       84             68             30      106  31.9   \n",
            "552            6      114             88              0        0  27.8   \n",
            "553            1       88             62             24       44  29.9   \n",
            "554            1       84             64             23      115  36.9   \n",
            "555            7      124             70             33      215  25.5   \n",
            "556            1       97             70             40        0  38.1   \n",
            "557            8      110             76              0        0  27.8   \n",
            "558           11      103             68             40        0  46.2   \n",
            "559           11       85             74              0        0  30.1   \n",
            "560            6      125             76              0        0  33.8   \n",
            "561            0      198             66             32      274  41.3   \n",
            "562            1       87             68             34       77  37.6   \n",
            "563            6       99             60             19       54  26.9   \n",
            "564            0       91             80              0        0  32.4   \n",
            "565            2       95             54             14       88  26.1   \n",
            "566            1       99             72             30       18  38.6   \n",
            "567            6       92             62             32      126  32.0   \n",
            "568            4      154             72             29      126  31.3   \n",
            "569            0      121             66             30      165  34.3   \n",
            "570            3       78             70              0        0  32.5   \n",
            "571            2      130             96              0        0  22.6   \n",
            "572            3      111             58             31       44  29.5   \n",
            "573            2       98             60             17      120  34.7   \n",
            "574            1      143             86             30      330  30.1   \n",
            "575            1      119             44             47       63  35.5   \n",
            "576            6      108             44             20      130  24.0   \n",
            "577            2      118             80              0        0  42.9   \n",
            "578           10      133             68              0        0  27.0   \n",
            "579            2      197             70             99        0  34.7   \n",
            "580            0      151             90             46        0  42.1   \n",
            "581            6      109             60             27        0  25.0   \n",
            "582           12      121             78             17        0  26.5   \n",
            "583            8      100             76              0        0  38.7   \n",
            "584            8      124             76             24      600  28.7   \n",
            "585            1       93             56             11        0  22.5   \n",
            "586            8      143             66              0        0  34.9   \n",
            "587            6      103             66              0        0  24.3   \n",
            "588            3      176             86             27      156  33.3   \n",
            "589            0       73              0              0        0  21.1   \n",
            "590           11      111             84             40        0  46.8   \n",
            "591            2      112             78             50      140  39.4   \n",
            "592            3      132             80              0        0  34.4   \n",
            "593            2       82             52             22      115  28.5   \n",
            "594            6      123             72             45      230  33.6   \n",
            "595            0      188             82             14      185  32.0   \n",
            "596            0       67             76              0        0  45.3   \n",
            "597            1       89             24             19       25  27.8   \n",
            "598            1      173             74              0        0  36.8   \n",
            "599            1      109             38             18      120  23.1   \n",
            "600            1      108             88             19        0  27.1   \n",
            "601            6       96              0              0        0  23.7   \n",
            "602            1      124             74             36        0  27.8   \n",
            "603            7      150             78             29      126  35.2   \n",
            "604            4      183              0              0        0  28.4   \n",
            "605            1      124             60             32        0  35.8   \n",
            "606            1      181             78             42      293  40.0   \n",
            "607            1       92             62             25       41  19.5   \n",
            "608            0      152             82             39      272  41.5   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "614           11      138             74             26      144  36.1   \n",
            "615            3      106             72              0        0  25.8   \n",
            "616            6      117             96              0        0  28.7   \n",
            "617            2       68             62             13       15  20.1   \n",
            "618            9      112             82             24        0  28.2   \n",
            "619            0      119              0              0        0  32.4   \n",
            "620            2      112             86             42      160  38.4   \n",
            "621            2       92             76             20        0  24.2   \n",
            "622            6      183             94              0        0  40.8   \n",
            "623            0       94             70             27      115  43.5   \n",
            "624            2      108             64              0        0  30.8   \n",
            "625            4       90             88             47       54  37.7   \n",
            "626            0      125             68              0        0  24.7   \n",
            "627            0      132             78              0        0  32.4   \n",
            "628            5      128             80              0        0  34.6   \n",
            "629            4       94             65             22        0  24.7   \n",
            "630            7      114             64              0        0  27.4   \n",
            "631            0      102             78             40       90  34.5   \n",
            "632            2      111             60              0        0  26.2   \n",
            "633            1      128             82             17      183  27.5   \n",
            "634           10       92             62              0        0  25.9   \n",
            "635           13      104             72              0        0  31.2   \n",
            "636            5      104             74              0        0  28.8   \n",
            "637            2       94             76             18       66  31.6   \n",
            "638            7       97             76             32       91  40.9   \n",
            "639            1      100             74             12       46  19.5   \n",
            "640            0      102             86             17      105  29.3   \n",
            "641            4      128             70              0        0  34.3   \n",
            "642            6      147             80              0        0  29.5   \n",
            "643            4       90              0              0        0  28.0   \n",
            "644            3      103             72             30      152  27.6   \n",
            "645            2      157             74             35      440  39.4   \n",
            "646            1      167             74             17      144  23.4   \n",
            "647            0      179             50             36      159  37.8   \n",
            "648           11      136             84             35      130  28.3   \n",
            "649            0      107             60             25        0  26.4   \n",
            "650            1       91             54             25      100  25.2   \n",
            "651            1      117             60             23      106  33.8   \n",
            "652            5      123             74             40       77  34.1   \n",
            "653            2      120             54              0        0  26.8   \n",
            "654            1      106             70             28      135  34.2   \n",
            "655            2      155             52             27      540  38.7   \n",
            "656            2      101             58             35       90  21.8   \n",
            "657            1      120             80             48      200  38.9   \n",
            "658           11      127            106              0        0  39.0   \n",
            "659            3       80             82             31       70  34.2   \n",
            "660           10      162             84              0        0  27.7   \n",
            "661            1      199             76             43        0  42.9   \n",
            "662            8      167            106             46      231  37.6   \n",
            "663            9      145             80             46      130  37.9   \n",
            "664            6      115             60             39        0  33.7   \n",
            "665            1      112             80             45      132  34.8   \n",
            "666            4      145             82             18        0  32.5   \n",
            "667           10      111             70             27        0  27.5   \n",
            "668            6       98             58             33      190  34.0   \n",
            "669            9      154             78             30      100  30.9   \n",
            "670            6      165             68             26      168  33.6   \n",
            "671            1       99             58             10        0  25.4   \n",
            "672           10       68            106             23       49  35.5   \n",
            "673            3      123            100             35      240  57.3   \n",
            "674            8       91             82              0        0  35.6   \n",
            "675            6      195             70              0        0  30.9   \n",
            "676            9      156             86              0        0  24.8   \n",
            "677            0       93             60              0        0  35.3   \n",
            "678            3      121             52              0        0  36.0   \n",
            "679            2      101             58             17      265  24.2   \n",
            "680            2       56             56             28       45  24.2   \n",
            "681            0      162             76             36        0  49.6   \n",
            "682            0       95             64             39      105  44.6   \n",
            "683            4      125             80              0        0  32.3   \n",
            "684            5      136             82              0        0   0.0   \n",
            "685            2      129             74             26      205  33.2   \n",
            "686            3      130             64              0        0  23.1   \n",
            "687            1      107             50             19        0  28.3   \n",
            "688            1      140             74             26      180  24.1   \n",
            "689            1      144             82             46      180  46.1   \n",
            "690            8      107             80              0        0  24.6   \n",
            "691           13      158            114              0        0  42.3   \n",
            "692            2      121             70             32       95  39.1   \n",
            "693            7      129             68             49      125  38.5   \n",
            "694            2       90             60              0        0  23.5   \n",
            "695            7      142             90             24      480  30.4   \n",
            "696            3      169             74             19      125  29.9   \n",
            "697            0       99              0              0        0  25.0   \n",
            "698            4      127             88             11      155  34.5   \n",
            "699            4      118             70              0        0  44.5   \n",
            "700            2      122             76             27      200  35.9   \n",
            "701            6      125             78             31        0  27.6   \n",
            "702            1      168             88             29        0  35.0   \n",
            "703            2      129              0              0        0  38.5   \n",
            "704            4      110             76             20      100  28.4   \n",
            "705            6       80             80             36        0  39.8   \n",
            "706           10      115              0              0        0   0.0   \n",
            "707            2      127             46             21      335  34.4   \n",
            "708            9      164             78              0        0  32.8   \n",
            "709            2       93             64             32      160  38.0   \n",
            "710            3      158             64             13      387  31.2   \n",
            "711            5      126             78             27       22  29.6   \n",
            "712           10      129             62             36        0  41.2   \n",
            "713            0      134             58             20      291  26.4   \n",
            "714            3      102             74              0        0  29.5   \n",
            "715            7      187             50             33      392  33.9   \n",
            "716            3      173             78             39      185  33.8   \n",
            "717           10       94             72             18        0  23.1   \n",
            "718            1      108             60             46      178  35.5   \n",
            "719            5       97             76             27        0  35.6   \n",
            "720            4       83             86             19        0  29.3   \n",
            "721            1      114             66             36      200  38.1   \n",
            "722            1      149             68             29      127  29.3   \n",
            "723            5      117             86             30      105  39.1   \n",
            "724            1      111             94              0        0  32.8   \n",
            "725            4      112             78             40        0  39.4   \n",
            "726            1      116             78             29      180  36.1   \n",
            "727            0      141             84             26        0  32.4   \n",
            "728            2      175             88              0        0  22.9   \n",
            "729            2       92             52              0        0  30.1   \n",
            "730            3      130             78             23       79  28.4   \n",
            "731            8      120             86              0        0  28.4   \n",
            "732            2      174             88             37      120  44.5   \n",
            "733            2      106             56             27      165  29.0   \n",
            "734            2      105             75              0        0  23.3   \n",
            "735            4       95             60             32        0  35.4   \n",
            "736            0      126             86             27      120  27.4   \n",
            "737            8       65             72             23        0  32.0   \n",
            "738            2       99             60             17      160  36.6   \n",
            "739            1      102             74              0        0  39.5   \n",
            "740           11      120             80             37      150  42.3   \n",
            "741            3      102             44             20       94  30.8   \n",
            "742            1      109             58             18      116  28.5   \n",
            "743            9      140             94              0        0  32.7   \n",
            "744           13      153             88             37      140  40.6   \n",
            "745           12      100             84             33      105  30.0   \n",
            "746            1      147             94             41        0  49.3   \n",
            "747            1       81             74             41       57  46.3   \n",
            "748            3      187             70             22      200  36.4   \n",
            "749            6      162             62              0        0  24.3   \n",
            "750            4      136             70              0        0  31.2   \n",
            "751            1      121             78             39       74  39.0   \n",
            "752            3      108             62             24        0  26.0   \n",
            "753            0      181             88             44      510  43.3   \n",
            "754            8      154             78             32        0  32.4   \n",
            "755            1      128             88             39      110  36.5   \n",
            "756            7      137             90             41        0  32.0   \n",
            "757            0      123             72              0        0  36.3   \n",
            "758            1      106             76              0        0  37.5   \n",
            "759            6      190             92              0        0  35.5   \n",
            "760            2       88             58             26       16  28.4   \n",
            "761            9      170             74             31        0  44.0   \n",
            "762            9       89             62              0        0  22.5   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "5                       0.201   30        0  \n",
            "6                       0.248   26        1  \n",
            "7                       0.134   29        0  \n",
            "8                       0.158   53        1  \n",
            "9                       0.232   54        1  \n",
            "10                      0.191   30        0  \n",
            "11                      0.537   34        1  \n",
            "12                      1.441   57        0  \n",
            "13                      0.398   59        1  \n",
            "14                      0.587   51        1  \n",
            "15                      0.484   32        1  \n",
            "16                      0.551   31        1  \n",
            "17                      0.254   31        1  \n",
            "18                      0.183   33        0  \n",
            "19                      0.529   32        1  \n",
            "20                      0.704   27        0  \n",
            "21                      0.388   50        0  \n",
            "22                      0.451   41        1  \n",
            "23                      0.263   29        1  \n",
            "24                      0.254   51        1  \n",
            "25                      0.205   41        1  \n",
            "26                      0.257   43        1  \n",
            "27                      0.487   22        0  \n",
            "28                      0.245   57        0  \n",
            "29                      0.337   38        0  \n",
            "30                      0.546   60        0  \n",
            "31                      0.851   28        1  \n",
            "32                      0.267   22        0  \n",
            "33                      0.188   28        0  \n",
            "34                      0.512   45        0  \n",
            "35                      0.966   33        0  \n",
            "36                      0.420   35        0  \n",
            "37                      0.665   46        1  \n",
            "38                      0.503   27        1  \n",
            "39                      1.390   56        1  \n",
            "40                      0.271   26        0  \n",
            "41                      0.696   37        0  \n",
            "42                      0.235   48        0  \n",
            "43                      0.721   54        1  \n",
            "44                      0.294   40        0  \n",
            "45                      1.893   25        1  \n",
            "46                      0.564   29        0  \n",
            "47                      0.586   22        0  \n",
            "48                      0.344   31        1  \n",
            "49                      0.305   24        0  \n",
            "50                      0.491   22        0  \n",
            "51                      0.526   26        0  \n",
            "52                      0.342   30        0  \n",
            "53                      0.467   58        1  \n",
            "54                      0.718   42        0  \n",
            "55                      0.248   21        0  \n",
            "56                      0.254   41        1  \n",
            "57                      0.962   31        0  \n",
            "58                      1.781   44        0  \n",
            "59                      0.173   22        0  \n",
            "60                      0.304   21        0  \n",
            "61                      0.270   39        1  \n",
            "62                      0.587   36        0  \n",
            "63                      0.699   24        0  \n",
            "64                      0.258   42        1  \n",
            "65                      0.203   32        0  \n",
            "66                      0.855   38        1  \n",
            "67                      0.845   54        0  \n",
            "68                      0.334   25        0  \n",
            "69                      0.189   27        0  \n",
            "70                      0.867   28        1  \n",
            "71                      0.411   26        0  \n",
            "72                      0.583   42        1  \n",
            "73                      0.231   23        0  \n",
            "74                      0.396   22        0  \n",
            "75                      0.140   22        0  \n",
            "76                      0.391   41        0  \n",
            "77                      0.370   27        0  \n",
            "78                      0.270   26        1  \n",
            "79                      0.307   24        0  \n",
            "80                      0.140   22        0  \n",
            "81                      0.102   22        0  \n",
            "82                      0.767   36        0  \n",
            "83                      0.237   22        0  \n",
            "84                      0.227   37        1  \n",
            "85                      0.698   27        0  \n",
            "86                      0.178   45        0  \n",
            "87                      0.324   26        0  \n",
            "88                      0.153   43        1  \n",
            "89                      0.165   24        0  \n",
            "90                      0.258   21        0  \n",
            "91                      0.443   34        0  \n",
            "92                      0.261   42        0  \n",
            "93                      0.277   60        1  \n",
            "94                      0.761   21        0  \n",
            "95                      0.255   40        0  \n",
            "96                      0.130   24        0  \n",
            "97                      0.323   22        0  \n",
            "98                      0.356   23        0  \n",
            "99                      0.325   31        1  \n",
            "100                     1.222   33        1  \n",
            "101                     0.179   22        0  \n",
            "102                     0.262   21        0  \n",
            "103                     0.283   24        0  \n",
            "104                     0.930   27        0  \n",
            "105                     0.801   21        0  \n",
            "106                     0.207   27        0  \n",
            "107                     0.287   37        0  \n",
            "108                     0.336   25        0  \n",
            "109                     0.247   24        1  \n",
            "110                     0.199   24        1  \n",
            "111                     0.543   46        1  \n",
            "112                     0.192   23        0  \n",
            "113                     0.391   25        0  \n",
            "114                     0.588   39        1  \n",
            "115                     0.539   61        1  \n",
            "116                     0.220   38        1  \n",
            "117                     0.654   25        0  \n",
            "118                     0.443   22        0  \n",
            "119                     0.223   21        0  \n",
            "120                     0.759   25        1  \n",
            "121                     0.260   24        0  \n",
            "122                     0.404   23        0  \n",
            "123                     0.186   69        0  \n",
            "124                     0.278   23        1  \n",
            "125                     0.496   26        1  \n",
            "126                     0.452   30        0  \n",
            "127                     0.261   23        0  \n",
            "128                     0.403   40        1  \n",
            "129                     0.741   62        1  \n",
            "130                     0.361   33        1  \n",
            "131                     1.114   33        1  \n",
            "132                     0.356   30        1  \n",
            "133                     0.457   39        0  \n",
            "134                     0.647   26        0  \n",
            "135                     0.088   31        0  \n",
            "136                     0.597   21        0  \n",
            "137                     0.532   22        0  \n",
            "138                     0.703   29        0  \n",
            "139                     0.159   28        0  \n",
            "140                     0.268   55        0  \n",
            "141                     0.286   38        0  \n",
            "142                     0.318   22        0  \n",
            "143                     0.272   42        1  \n",
            "144                     0.237   23        0  \n",
            "145                     0.572   21        0  \n",
            "146                     0.096   41        0  \n",
            "147                     1.400   34        0  \n",
            "148                     0.218   65        0  \n",
            "149                     0.085   22        0  \n",
            "150                     0.399   24        0  \n",
            "151                     0.432   37        0  \n",
            "152                     1.189   42        1  \n",
            "153                     0.687   23        0  \n",
            "154                     0.137   43        1  \n",
            "155                     0.337   36        1  \n",
            "156                     0.637   21        0  \n",
            "157                     0.833   23        0  \n",
            "158                     0.229   22        0  \n",
            "159                     0.817   47        1  \n",
            "160                     0.294   36        0  \n",
            "161                     0.204   45        0  \n",
            "162                     0.167   27        0  \n",
            "163                     0.368   21        0  \n",
            "164                     0.743   32        1  \n",
            "165                     0.722   41        1  \n",
            "166                     0.256   22        0  \n",
            "167                     0.709   34        0  \n",
            "168                     0.471   29        0  \n",
            "169                     0.495   29        0  \n",
            "170                     0.180   36        1  \n",
            "171                     0.542   29        1  \n",
            "172                     0.773   25        0  \n",
            "173                     0.678   23        0  \n",
            "174                     0.370   33        0  \n",
            "175                     0.719   36        1  \n",
            "176                     0.382   42        0  \n",
            "177                     0.319   26        1  \n",
            "178                     0.190   47        0  \n",
            "179                     0.956   37        1  \n",
            "180                     0.084   32        0  \n",
            "181                     0.725   23        0  \n",
            "182                     0.299   21        0  \n",
            "183                     0.268   27        0  \n",
            "184                     0.244   40        0  \n",
            "185                     0.745   41        1  \n",
            "186                     0.615   60        1  \n",
            "187                     1.321   33        1  \n",
            "188                     0.640   31        1  \n",
            "189                     0.361   25        1  \n",
            "190                     0.142   21        0  \n",
            "191                     0.374   40        0  \n",
            "192                     0.383   36        1  \n",
            "193                     0.578   40        1  \n",
            "194                     0.136   42        0  \n",
            "195                     0.395   29        1  \n",
            "196                     0.187   21        0  \n",
            "197                     0.678   23        1  \n",
            "198                     0.905   26        1  \n",
            "199                     0.150   29        1  \n",
            "200                     0.874   21        0  \n",
            "201                     0.236   28        0  \n",
            "202                     0.787   32        0  \n",
            "203                     0.235   27        0  \n",
            "204                     0.324   55        0  \n",
            "205                     0.407   27        0  \n",
            "206                     0.605   57        1  \n",
            "207                     0.151   52        1  \n",
            "208                     0.289   21        0  \n",
            "209                     0.355   41        1  \n",
            "210                     0.290   25        0  \n",
            "211                     0.375   24        0  \n",
            "212                     0.164   60        0  \n",
            "213                     0.431   24        1  \n",
            "214                     0.260   36        1  \n",
            "215                     0.742   38        1  \n",
            "216                     0.514   25        1  \n",
            "217                     0.464   32        0  \n",
            "218                     1.224   32        1  \n",
            "219                     0.261   41        1  \n",
            "220                     1.072   21        1  \n",
            "221                     0.805   66        1  \n",
            "222                     0.209   37        0  \n",
            "223                     0.687   61        0  \n",
            "224                     0.666   26        0  \n",
            "225                     0.101   22        0  \n",
            "226                     0.198   26        0  \n",
            "227                     0.652   24        1  \n",
            "228                     2.329   31        0  \n",
            "229                     0.089   24        0  \n",
            "230                     0.645   22        1  \n",
            "231                     0.238   46        1  \n",
            "232                     0.583   22        0  \n",
            "233                     0.394   29        0  \n",
            "234                     0.293   23        0  \n",
            "235                     0.479   26        1  \n",
            "236                     0.586   51        1  \n",
            "237                     0.686   23        1  \n",
            "238                     0.831   32        1  \n",
            "239                     0.582   27        0  \n",
            "240                     0.192   21        0  \n",
            "241                     0.446   22        0  \n",
            "242                     0.402   22        1  \n",
            "243                     1.318   33        1  \n",
            "244                     0.329   29        0  \n",
            "245                     1.213   49        1  \n",
            "246                     0.258   41        0  \n",
            "247                     0.427   23        0  \n",
            "248                     0.282   34        0  \n",
            "249                     0.143   23        0  \n",
            "250                     0.380   42        0  \n",
            "251                     0.284   27        0  \n",
            "252                     0.249   24        0  \n",
            "253                     0.238   25        0  \n",
            "254                     0.926   44        1  \n",
            "255                     0.543   21        1  \n",
            "256                     0.557   30        0  \n",
            "257                     0.092   25        0  \n",
            "258                     0.655   24        0  \n",
            "259                     1.353   51        1  \n",
            "260                     0.299   34        0  \n",
            "261                     0.761   27        1  \n",
            "262                     0.612   24        0  \n",
            "263                     0.200   63        0  \n",
            "264                     0.226   35        1  \n",
            "265                     0.997   43        0  \n",
            "266                     0.933   25        1  \n",
            "267                     1.101   24        0  \n",
            "268                     0.078   21        0  \n",
            "269                     0.240   28        1  \n",
            "270                     1.136   38        1  \n",
            "271                     0.128   21        0  \n",
            "272                     0.254   40        0  \n",
            "273                     0.422   21        0  \n",
            "274                     0.251   52        0  \n",
            "275                     0.677   25        0  \n",
            "276                     0.296   29        1  \n",
            "277                     0.454   23        0  \n",
            "278                     0.744   57        0  \n",
            "279                     0.881   22        0  \n",
            "280                     0.334   28        1  \n",
            "281                     0.280   39        0  \n",
            "282                     0.262   37        0  \n",
            "283                     0.165   47        1  \n",
            "284                     0.259   52        1  \n",
            "285                     0.647   51        0  \n",
            "286                     0.619   34        0  \n",
            "287                     0.808   29        1  \n",
            "288                     0.340   26        0  \n",
            "289                     0.263   33        0  \n",
            "290                     0.434   21        0  \n",
            "291                     0.757   25        1  \n",
            "292                     1.224   31        1  \n",
            "293                     0.613   24        1  \n",
            "294                     0.254   65        0  \n",
            "295                     0.692   28        0  \n",
            "296                     0.337   29        1  \n",
            "297                     0.520   24        0  \n",
            "298                     0.412   46        1  \n",
            "299                     0.840   58        0  \n",
            "300                     0.839   30        1  \n",
            "301                     0.422   25        1  \n",
            "302                     0.156   35        0  \n",
            "303                     0.209   28        1  \n",
            "304                     0.207   37        0  \n",
            "305                     0.215   29        0  \n",
            "306                     0.326   47        1  \n",
            "307                     0.143   21        0  \n",
            "308                     1.391   25        1  \n",
            "309                     0.875   30        1  \n",
            "310                     0.313   41        0  \n",
            "311                     0.605   22        0  \n",
            "312                     0.433   27        1  \n",
            "313                     0.626   25        0  \n",
            "314                     1.127   43        1  \n",
            "315                     0.315   26        0  \n",
            "316                     0.284   30        0  \n",
            "317                     0.345   29        1  \n",
            "318                     0.150   28        0  \n",
            "319                     0.129   59        1  \n",
            "320                     0.527   31        0  \n",
            "321                     0.197   25        1  \n",
            "322                     0.254   36        1  \n",
            "323                     0.731   43        1  \n",
            "324                     0.148   21        0  \n",
            "325                     0.123   24        0  \n",
            "326                     0.692   30        1  \n",
            "327                     0.200   37        0  \n",
            "328                     0.127   23        1  \n",
            "329                     0.122   37        0  \n",
            "330                     1.476   46        0  \n",
            "331                     0.166   25        0  \n",
            "332                     0.282   41        1  \n",
            "333                     0.137   44        0  \n",
            "334                     0.260   22        0  \n",
            "335                     0.259   26        0  \n",
            "336                     0.932   44        0  \n",
            "337                     0.343   44        1  \n",
            "338                     0.893   33        1  \n",
            "339                     0.331   41        1  \n",
            "340                     0.472   22        0  \n",
            "341                     0.673   36        0  \n",
            "342                     0.389   22        0  \n",
            "343                     0.290   33        0  \n",
            "344                     0.485   57        0  \n",
            "345                     0.349   49        0  \n",
            "346                     0.654   22        0  \n",
            "347                     0.187   23        0  \n",
            "348                     0.279   26        0  \n",
            "349                     0.346   37        1  \n",
            "350                     0.237   29        0  \n",
            "351                     0.252   30        0  \n",
            "352                     0.243   46        0  \n",
            "353                     0.580   24        0  \n",
            "354                     0.559   21        0  \n",
            "355                     0.302   49        1  \n",
            "356                     0.962   28        1  \n",
            "357                     0.569   44        1  \n",
            "358                     0.378   48        0  \n",
            "359                     0.875   29        1  \n",
            "360                     0.583   29        1  \n",
            "361                     0.207   63        0  \n",
            "362                     0.305   65        0  \n",
            "363                     0.520   67        1  \n",
            "364                     0.385   30        0  \n",
            "365                     0.499   30        0  \n",
            "366                     0.368   29        1  \n",
            "367                     0.252   21        0  \n",
            "368                     0.306   22        0  \n",
            "369                     0.234   45        1  \n",
            "370                     2.137   25        1  \n",
            "371                     1.731   21        0  \n",
            "372                     0.545   21        0  \n",
            "373                     0.225   25        0  \n",
            "374                     0.816   28        0  \n",
            "375                     0.528   58        1  \n",
            "376                     0.299   22        0  \n",
            "377                     0.509   22        0  \n",
            "378                     0.238   32        1  \n",
            "379                     1.021   35        0  \n",
            "380                     0.821   24        0  \n",
            "381                     0.236   22        0  \n",
            "382                     0.947   21        0  \n",
            "383                     1.268   25        0  \n",
            "384                     0.221   25        0  \n",
            "385                     0.205   24        0  \n",
            "386                     0.660   35        1  \n",
            "387                     0.239   45        1  \n",
            "388                     0.452   58        1  \n",
            "389                     0.949   28        0  \n",
            "390                     0.444   42        0  \n",
            "391                     0.340   27        1  \n",
            "392                     0.389   21        0  \n",
            "393                     0.463   37        0  \n",
            "394                     0.803   31        1  \n",
            "395                     1.600   25        0  \n",
            "396                     0.944   39        0  \n",
            "397                     0.196   22        1  \n",
            "398                     0.389   25        0  \n",
            "399                     0.241   25        1  \n",
            "400                     0.161   31        1  \n",
            "401                     0.151   55        0  \n",
            "402                     0.286   35        1  \n",
            "403                     0.280   38        0  \n",
            "404                     0.135   41        1  \n",
            "405                     0.520   26        0  \n",
            "406                     0.376   46        1  \n",
            "407                     0.336   25        0  \n",
            "408                     1.191   39        1  \n",
            "409                     0.702   28        1  \n",
            "410                     0.674   28        0  \n",
            "411                     0.528   25        0  \n",
            "412                     1.076   22        0  \n",
            "413                     0.256   21        0  \n",
            "414                     0.534   21        1  \n",
            "415                     0.258   22        1  \n",
            "416                     1.095   22        0  \n",
            "417                     0.554   37        1  \n",
            "418                     0.624   27        0  \n",
            "419                     0.219   28        1  \n",
            "420                     0.507   26        0  \n",
            "421                     0.561   21        0  \n",
            "422                     0.496   21        0  \n",
            "423                     0.421   21        0  \n",
            "424                     0.516   36        1  \n",
            "425                     0.264   31        1  \n",
            "426                     0.256   25        0  \n",
            "427                     0.328   38        1  \n",
            "428                     0.284   26        0  \n",
            "429                     0.233   43        1  \n",
            "430                     0.108   23        0  \n",
            "431                     0.551   38        0  \n",
            "432                     0.527   22        0  \n",
            "433                     0.167   29        0  \n",
            "434                     1.138   36        0  \n",
            "435                     0.205   29        1  \n",
            "436                     0.244   41        0  \n",
            "437                     0.434   28        0  \n",
            "438                     0.147   21        0  \n",
            "439                     0.727   31        0  \n",
            "440                     0.435   41        1  \n",
            "441                     0.497   22        0  \n",
            "442                     0.230   24        0  \n",
            "443                     0.955   33        1  \n",
            "444                     0.380   30        1  \n",
            "445                     2.420   25        1  \n",
            "446                     0.658   28        0  \n",
            "447                     0.330   26        0  \n",
            "448                     0.510   22        1  \n",
            "449                     0.285   26        0  \n",
            "450                     0.415   23        0  \n",
            "451                     0.542   23        1  \n",
            "452                     0.381   25        0  \n",
            "453                     0.832   72        0  \n",
            "454                     0.498   24        0  \n",
            "455                     0.212   38        1  \n",
            "456                     0.687   62        0  \n",
            "457                     0.364   24        0  \n",
            "458                     1.001   51        1  \n",
            "459                     0.460   81        0  \n",
            "460                     0.733   48        0  \n",
            "461                     0.416   26        0  \n",
            "462                     0.705   39        0  \n",
            "463                     0.258   37        0  \n",
            "464                     1.022   34        0  \n",
            "465                     0.452   21        0  \n",
            "466                     0.269   22        0  \n",
            "467                     0.600   25        0  \n",
            "468                     0.183   38        1  \n",
            "469                     0.571   27        0  \n",
            "470                     0.607   28        0  \n",
            "471                     0.170   22        0  \n",
            "472                     0.259   22        0  \n",
            "473                     0.210   50        0  \n",
            "474                     0.126   24        0  \n",
            "475                     0.231   59        0  \n",
            "476                     0.711   29        1  \n",
            "477                     0.466   31        0  \n",
            "478                     0.162   39        0  \n",
            "479                     0.419   63        0  \n",
            "480                     0.344   35        1  \n",
            "481                     0.197   29        0  \n",
            "482                     0.306   28        0  \n",
            "483                     0.233   23        0  \n",
            "484                     0.630   31        1  \n",
            "485                     0.365   24        1  \n",
            "486                     0.536   21        0  \n",
            "487                     1.159   58        0  \n",
            "488                     0.294   28        0  \n",
            "489                     0.551   67        0  \n",
            "490                     0.629   24        0  \n",
            "491                     0.292   42        0  \n",
            "492                     0.145   33        0  \n",
            "493                     1.144   45        1  \n",
            "494                     0.174   22        0  \n",
            "495                     0.304   66        0  \n",
            "496                     0.292   30        0  \n",
            "497                     0.547   25        0  \n",
            "498                     0.163   55        1  \n",
            "499                     0.839   39        0  \n",
            "500                     0.313   21        0  \n",
            "501                     0.267   28        0  \n",
            "502                     0.727   41        1  \n",
            "503                     0.738   41        0  \n",
            "504                     0.238   40        0  \n",
            "505                     0.263   38        0  \n",
            "506                     0.314   35        1  \n",
            "507                     0.692   21        0  \n",
            "508                     0.968   21        0  \n",
            "509                     0.409   64        0  \n",
            "510                     0.297   46        1  \n",
            "511                     0.207   21        0  \n",
            "512                     0.200   58        0  \n",
            "513                     0.525   22        0  \n",
            "514                     0.154   24        0  \n",
            "515                     0.268   28        1  \n",
            "516                     0.771   53        1  \n",
            "517                     0.304   51        0  \n",
            "518                     0.180   41        0  \n",
            "519                     0.582   60        0  \n",
            "520                     0.187   25        0  \n",
            "521                     0.305   26        0  \n",
            "522                     0.189   26        0  \n",
            "523                     0.652   45        1  \n",
            "524                     0.151   24        0  \n",
            "525                     0.444   21        0  \n",
            "526                     0.299   21        0  \n",
            "527                     0.107   24        0  \n",
            "528                     0.493   22        0  \n",
            "529                     0.660   31        0  \n",
            "530                     0.717   22        0  \n",
            "531                     0.686   24        0  \n",
            "532                     0.917   29        0  \n",
            "533                     0.501   31        0  \n",
            "534                     1.251   24        0  \n",
            "535                     0.302   23        1  \n",
            "536                     0.197   46        0  \n",
            "537                     0.735   67        0  \n",
            "538                     0.804   23        0  \n",
            "539                     0.968   32        1  \n",
            "540                     0.661   43        1  \n",
            "541                     0.549   27        1  \n",
            "542                     0.825   56        1  \n",
            "543                     0.159   25        0  \n",
            "544                     0.365   29        0  \n",
            "545                     0.423   37        1  \n",
            "546                     1.034   53        1  \n",
            "547                     0.160   28        0  \n",
            "548                     0.341   50        0  \n",
            "549                     0.680   37        0  \n",
            "550                     0.204   21        0  \n",
            "551                     0.591   25        0  \n",
            "552                     0.247   66        0  \n",
            "553                     0.422   23        0  \n",
            "554                     0.471   28        0  \n",
            "555                     0.161   37        0  \n",
            "556                     0.218   30        0  \n",
            "557                     0.237   58        0  \n",
            "558                     0.126   42        0  \n",
            "559                     0.300   35        0  \n",
            "560                     0.121   54        1  \n",
            "561                     0.502   28        1  \n",
            "562                     0.401   24        0  \n",
            "563                     0.497   32        0  \n",
            "564                     0.601   27        0  \n",
            "565                     0.748   22        0  \n",
            "566                     0.412   21        0  \n",
            "567                     0.085   46        0  \n",
            "568                     0.338   37        0  \n",
            "569                     0.203   33        1  \n",
            "570                     0.270   39        0  \n",
            "571                     0.268   21        0  \n",
            "572                     0.430   22        0  \n",
            "573                     0.198   22        0  \n",
            "574                     0.892   23        0  \n",
            "575                     0.280   25        0  \n",
            "576                     0.813   35        0  \n",
            "577                     0.693   21        1  \n",
            "578                     0.245   36        0  \n",
            "579                     0.575   62        1  \n",
            "580                     0.371   21        1  \n",
            "581                     0.206   27        0  \n",
            "582                     0.259   62        0  \n",
            "583                     0.190   42        0  \n",
            "584                     0.687   52        1  \n",
            "585                     0.417   22        0  \n",
            "586                     0.129   41        1  \n",
            "587                     0.249   29        0  \n",
            "588                     1.154   52        1  \n",
            "589                     0.342   25        0  \n",
            "590                     0.925   45        1  \n",
            "591                     0.175   24        0  \n",
            "592                     0.402   44        1  \n",
            "593                     1.699   25        0  \n",
            "594                     0.733   34        0  \n",
            "595                     0.682   22        1  \n",
            "596                     0.194   46        0  \n",
            "597                     0.559   21        0  \n",
            "598                     0.088   38        1  \n",
            "599                     0.407   26        0  \n",
            "600                     0.400   24        0  \n",
            "601                     0.190   28        0  \n",
            "602                     0.100   30        0  \n",
            "603                     0.692   54        1  \n",
            "604                     0.212   36        1  \n",
            "605                     0.514   21        0  \n",
            "606                     1.258   22        1  \n",
            "607                     0.482   25        0  \n",
            "608                     0.270   27        0  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "614                     0.557   50        1  \n",
            "615                     0.207   27        0  \n",
            "616                     0.157   30        0  \n",
            "617                     0.257   23        0  \n",
            "618                     1.282   50        1  \n",
            "619                     0.141   24        1  \n",
            "620                     0.246   28        0  \n",
            "621                     1.698   28        0  \n",
            "622                     1.461   45        0  \n",
            "623                     0.347   21        0  \n",
            "624                     0.158   21        0  \n",
            "625                     0.362   29        0  \n",
            "626                     0.206   21        0  \n",
            "627                     0.393   21        0  \n",
            "628                     0.144   45        0  \n",
            "629                     0.148   21        0  \n",
            "630                     0.732   34        1  \n",
            "631                     0.238   24        0  \n",
            "632                     0.343   23        0  \n",
            "633                     0.115   22        0  \n",
            "634                     0.167   31        0  \n",
            "635                     0.465   38        1  \n",
            "636                     0.153   48        0  \n",
            "637                     0.649   23        0  \n",
            "638                     0.871   32        1  \n",
            "639                     0.149   28        0  \n",
            "640                     0.695   27        0  \n",
            "641                     0.303   24        0  \n",
            "642                     0.178   50        1  \n",
            "643                     0.610   31        0  \n",
            "644                     0.730   27        0  \n",
            "645                     0.134   30        0  \n",
            "646                     0.447   33        1  \n",
            "647                     0.455   22        1  \n",
            "648                     0.260   42        1  \n",
            "649                     0.133   23        0  \n",
            "650                     0.234   23        0  \n",
            "651                     0.466   27        0  \n",
            "652                     0.269   28        0  \n",
            "653                     0.455   27        0  \n",
            "654                     0.142   22        0  \n",
            "655                     0.240   25        1  \n",
            "656                     0.155   22        0  \n",
            "657                     1.162   41        0  \n",
            "658                     0.190   51        0  \n",
            "659                     1.292   27        1  \n",
            "660                     0.182   54        0  \n",
            "661                     1.394   22        1  \n",
            "662                     0.165   43        1  \n",
            "663                     0.637   40        1  \n",
            "664                     0.245   40        1  \n",
            "665                     0.217   24        0  \n",
            "666                     0.235   70        1  \n",
            "667                     0.141   40        1  \n",
            "668                     0.430   43        0  \n",
            "669                     0.164   45        0  \n",
            "670                     0.631   49        0  \n",
            "671                     0.551   21        0  \n",
            "672                     0.285   47        0  \n",
            "673                     0.880   22        0  \n",
            "674                     0.587   68        0  \n",
            "675                     0.328   31        1  \n",
            "676                     0.230   53        1  \n",
            "677                     0.263   25        0  \n",
            "678                     0.127   25        1  \n",
            "679                     0.614   23        0  \n",
            "680                     0.332   22        0  \n",
            "681                     0.364   26        1  \n",
            "682                     0.366   22        0  \n",
            "683                     0.536   27        1  \n",
            "684                     0.640   69        0  \n",
            "685                     0.591   25        0  \n",
            "686                     0.314   22        0  \n",
            "687                     0.181   29        0  \n",
            "688                     0.828   23        0  \n",
            "689                     0.335   46        1  \n",
            "690                     0.856   34        0  \n",
            "691                     0.257   44        1  \n",
            "692                     0.886   23        0  \n",
            "693                     0.439   43        1  \n",
            "694                     0.191   25        0  \n",
            "695                     0.128   43        1  \n",
            "696                     0.268   31        1  \n",
            "697                     0.253   22        0  \n",
            "698                     0.598   28        0  \n",
            "699                     0.904   26        0  \n",
            "700                     0.483   26        0  \n",
            "701                     0.565   49        1  \n",
            "702                     0.905   52        1  \n",
            "703                     0.304   41        0  \n",
            "704                     0.118   27        0  \n",
            "705                     0.177   28        0  \n",
            "706                     0.261   30        1  \n",
            "707                     0.176   22        0  \n",
            "708                     0.148   45        1  \n",
            "709                     0.674   23        1  \n",
            "710                     0.295   24        0  \n",
            "711                     0.439   40        0  \n",
            "712                     0.441   38        1  \n",
            "713                     0.352   21        0  \n",
            "714                     0.121   32        0  \n",
            "715                     0.826   34        1  \n",
            "716                     0.970   31        1  \n",
            "717                     0.595   56        0  \n",
            "718                     0.415   24        0  \n",
            "719                     0.378   52        1  \n",
            "720                     0.317   34        0  \n",
            "721                     0.289   21        0  \n",
            "722                     0.349   42        1  \n",
            "723                     0.251   42        0  \n",
            "724                     0.265   45        0  \n",
            "725                     0.236   38        0  \n",
            "726                     0.496   25        0  \n",
            "727                     0.433   22        0  \n",
            "728                     0.326   22        0  \n",
            "729                     0.141   22        0  \n",
            "730                     0.323   34        1  \n",
            "731                     0.259   22        1  \n",
            "732                     0.646   24        1  \n",
            "733                     0.426   22        0  \n",
            "734                     0.560   53        0  \n",
            "735                     0.284   28        0  \n",
            "736                     0.515   21        0  \n",
            "737                     0.600   42        0  \n",
            "738                     0.453   21        0  \n",
            "739                     0.293   42        1  \n",
            "740                     0.785   48        1  \n",
            "741                     0.400   26        0  \n",
            "742                     0.219   22        0  \n",
            "743                     0.734   45        1  \n",
            "744                     1.174   39        0  \n",
            "745                     0.488   46        0  \n",
            "746                     0.358   27        1  \n",
            "747                     1.096   32        0  \n",
            "748                     0.408   36        1  \n",
            "749                     0.178   50        1  \n",
            "750                     1.182   22        1  \n",
            "751                     0.261   28        0  \n",
            "752                     0.223   25        0  \n",
            "753                     0.222   26        1  \n",
            "754                     0.443   45        1  \n",
            "755                     1.057   37        1  \n",
            "756                     0.391   39        0  \n",
            "757                     0.258   52        1  \n",
            "758                     0.197   26        0  \n",
            "759                     0.278   66        1  \n",
            "760                     0.766   22        0  \n",
            "761                     0.403   43        1  \n",
            "762                     0.142   33        0  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing\n",
        "# Anggap 0 pada kolom tertentu adalah missing -> ganti dengan NaN\n",
        "for c in ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].replace(0, pd.NA)\n",
        "\n",
        "# Shuffle deterministik\n",
        "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# Split 70/15/15\n",
        "n = len(df)\n",
        "n_train = int(0.7 * n) #untuk training\n",
        "n_val = int(0.15 * n) # untuk menghindari overfitting\n",
        "train_df = df.iloc[:n_train].reset_index(drop=True)\n",
        "val_df = df.iloc[n_train:n_train + n_val].reset_index(drop=True)\n",
        "test_df = df.iloc[n_train + n_val:].reset_index(drop=True) # untuk mengukur performa terakhir\n",
        "\n",
        "# Pisah fitur dan label\n",
        "# model belajar dari fitur X untuk memprediksi outcome Y\n",
        "X_train_df = train_df.drop(columns=['Outcome'])\n",
        "y_train = train_df['Outcome'].astype(int).to_numpy()\n",
        "X_val_df = val_df.drop(columns=['Outcome'])\n",
        "y_val = val_df['Outcome'].astype(int).to_numpy()\n",
        "X_test_df = test_df.drop(columns=['Outcome'])\n",
        "y_test = test_df['Outcome'].astype(int).to_numpy()\n",
        "\n",
        "# Impute median berdasarkan training set (hindari leakage)\n",
        "# Gunakan median untuk mengisi missing\n",
        "medians = X_train_df.median()\n",
        "X_train_df = X_train_df.fillna(medians)\n",
        "X_val_df = X_val_df.fillna(medians)\n",
        "X_test_df = X_test_df.fillna(medians)\n",
        "\n",
        "# Convert ke numpy float32 / int32\n",
        "X_train = X_train_df.to_numpy(dtype=np.float32)\n",
        "X_val = X_val_df.to_numpy(dtype=np.float32)\n",
        "X_test = X_test_df.to_numpy(dtype=np.float32)\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_val = y_val.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "\n",
        "print(f\"Split -> train: {X_train.shape[0]}, val: {X_val.shape[0]}, test: {X_test.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dqrxxSF9Pp6",
        "outputId": "9f931fec-caa7-44b8-d48d-5258d88a24e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split -> train: 537, val: 115, test: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3725679787.py:30: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  X_train_df = X_train_df.fillna(medians)\n",
            "/tmp/ipython-input-3725679787.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  X_val_df = X_val_df.fillna(medians)\n",
            "/tmp/ipython-input-3725679787.py:32: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  X_test_df = X_test_df.fillna(medians)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Normalizer / Penyeimbang skala data sebelum model machine learning dilatih\n",
        "# Membuat layer normalisasi dari TensorFlow Keras\n",
        "# Fungsi: menyesuaikan skala tiap fitur agar punya mean=0 dan varians=1\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "\n",
        "# Adaptasi normalizer ke data training\n",
        "# TensorFlow akan menghitung mean dan variance tiap fitur\n",
        "# Pake X_train karena dihitung dari data pelatihan, bukan data validasi atau pengujian\n",
        "normalizer.adapt(X_train)\n",
        "\n",
        "# Tampilkan hasil normalisasi\n",
        "print(\">> Rata-rata (mean) tiap fitur:\")\n",
        "print(np.round(normalizer.mean.numpy(), 4))\n",
        "\n",
        "print(\"\\n>> Variansi (variance) tiap fitur:\")\n",
        "print(np.round(normalizer.variance.numpy(), 4))\n",
        "\n",
        "# Uji hasil normalisasi pada beberapa data awal\n",
        "X_train_norm = normalizer(X_train[:5])\n",
        "print(\"\\n>> 5 data pertama setelah normalisasi:\")\n",
        "print(np.round(X_train_norm.numpy(), 4))\n",
        "\n",
        "# Verifikasi bentuk data\n",
        "print(\"\\nShape data setelah normalisasi:\", X_train_norm.shape)\n",
        "print(\"Normalisasi selesai ✅\")"
      ],
      "metadata": {
        "id": "eaKu7O9_9iY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328989f7-6a75-4d7a-9dcb-97f79c5dcbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Rata-rata (mean) tiap fitur:\n",
            "[[  3.8305 121.9013  72.7989  29.1732 142.4795  32.6736   0.4725  33.4376]]\n",
            "\n",
            ">> Variansi (variance) tiap fitur:\n",
            "[[1.142190e+01 9.344503e+02 1.451998e+02 7.937970e+01 7.596972e+03\n",
            "  4.843830e+01 1.125000e-01 1.389388e+02]]\n",
            "\n",
            ">> 5 data pertama setelah normalisasi:\n",
            "[[ 0.6419 -0.7819 -1.2281  0.4295  0.5452  0.1906 -0.1268  0.8112]\n",
            " [-0.5416 -0.3239  0.1827  0.3173 -0.1948  0.4348 -0.9675 -1.0552]\n",
            " [-0.5416 -0.4548 -0.7302 -0.0194 -0.1948 -0.2692 -0.9377 -1.0552]\n",
            " [ 1.2337 -0.4875  0.5976 -0.0194 -0.1948 -1.16    1.1433  0.0477]\n",
            " [ 0.9378  0.4612  1.4275 -0.0194 -0.1948 -0.3985 -0.7827  1.4051]]\n",
            "\n",
            "Shape data setelah normalisasi: (5, 8)\n",
            "Normalisasi selesai ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. tf.data.datasets\n",
        "# data pelatihan, validasi, dan pengujian diubah jadi format tf.data.Dataset (format data pipeline bawaan TensorFlow)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
        "    .shuffle(1024, seed=SEED) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "khlBUMf-9qIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Model\n",
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')   # inti klasifikasi -> probabilitas 0..1\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "model.build(input_shape=(None, X_train.shape[1]))\n",
        "print(\"\\nArsitektur model singkat:\")\n",
        "model.summary()\n",
        "\n",
        "# rumus param = (m x n) + n\n",
        "# yg trainable: dense dan batchnorm\n",
        "# non trainable: batchnormalization, layer spt normalization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "mPNjzaA89xFn",
        "outputId": "543eae18-95b3-4970-a984-9dad85df81af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arsitektur model singkat:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization (\u001b[38;5;33mNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,962\u001b[0m (11.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,962</span> (11.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m145\u001b[0m (584.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> (584.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train dengan callbacks\n",
        "early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=12,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ckpt_path = \"best_model.keras\"  # langsung nama file aja\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(\n",
        "    ckpt_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nMulai training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early, mc],\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1wwq2E97ZX",
        "outputId": "0c2af95f-7772-464b-abb4-0fb6c85fdd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mulai training...\n",
            "Epoch 1/200\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.67349, saving model to best_model.keras\n",
            "17/17 - 3s - 191ms/step - accuracy: 0.5680 - auc: 0.5980 - loss: 0.7040 - val_accuracy: 0.6435 - val_auc: 0.6543 - val_loss: 0.6735\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 2: val_loss improved from 0.67349 to 0.60993, saving model to best_model.keras\n",
            "17/17 - 0s - 7ms/step - accuracy: 0.6760 - auc: 0.7463 - loss: 0.5751 - val_accuracy: 0.6957 - val_auc: 0.7246 - val_loss: 0.6099\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 3: val_loss improved from 0.60993 to 0.57652, saving model to best_model.keras\n",
            "17/17 - 0s - 7ms/step - accuracy: 0.7058 - auc: 0.7791 - loss: 0.5323 - val_accuracy: 0.7130 - val_auc: 0.7532 - val_loss: 0.5765\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 4: val_loss improved from 0.57652 to 0.55680, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7505 - auc: 0.8148 - loss: 0.4958 - val_accuracy: 0.7391 - val_auc: 0.7663 - val_loss: 0.5568\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 5: val_loss improved from 0.55680 to 0.53967, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7523 - auc: 0.8223 - loss: 0.4839 - val_accuracy: 0.7304 - val_auc: 0.7752 - val_loss: 0.5397\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 6: val_loss improved from 0.53967 to 0.52858, saving model to best_model.keras\n",
            "17/17 - 0s - 20ms/step - accuracy: 0.7821 - auc: 0.8403 - loss: 0.4672 - val_accuracy: 0.7391 - val_auc: 0.7839 - val_loss: 0.5286\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 7: val_loss improved from 0.52858 to 0.51685, saving model to best_model.keras\n",
            "17/17 - 0s - 7ms/step - accuracy: 0.7728 - auc: 0.8423 - loss: 0.4650 - val_accuracy: 0.7391 - val_auc: 0.7936 - val_loss: 0.5168\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 8: val_loss improved from 0.51685 to 0.51009, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7728 - auc: 0.8484 - loss: 0.4574 - val_accuracy: 0.7304 - val_auc: 0.7979 - val_loss: 0.5101\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 9: val_loss improved from 0.51009 to 0.50726, saving model to best_model.keras\n",
            "17/17 - 0s - 8ms/step - accuracy: 0.7672 - auc: 0.8372 - loss: 0.4651 - val_accuracy: 0.7304 - val_auc: 0.7941 - val_loss: 0.5073\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 10: val_loss improved from 0.50726 to 0.50363, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7914 - auc: 0.8527 - loss: 0.4490 - val_accuracy: 0.7304 - val_auc: 0.7939 - val_loss: 0.5036\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 11: val_loss improved from 0.50363 to 0.50275, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7933 - auc: 0.8611 - loss: 0.4437 - val_accuracy: 0.7565 - val_auc: 0.7957 - val_loss: 0.5028\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 12: val_loss improved from 0.50275 to 0.49897, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7896 - auc: 0.8637 - loss: 0.4386 - val_accuracy: 0.7478 - val_auc: 0.7998 - val_loss: 0.4990\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 13: val_loss improved from 0.49897 to 0.49717, saving model to best_model.keras\n",
            "17/17 - 0s - 8ms/step - accuracy: 0.7989 - auc: 0.8662 - loss: 0.4328 - val_accuracy: 0.7391 - val_auc: 0.8004 - val_loss: 0.4972\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 14: val_loss improved from 0.49717 to 0.49663, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.7896 - auc: 0.8581 - loss: 0.4429 - val_accuracy: 0.7391 - val_auc: 0.7989 - val_loss: 0.4966\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 15: val_loss improved from 0.49663 to 0.49323, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.8063 - auc: 0.8649 - loss: 0.4314 - val_accuracy: 0.7391 - val_auc: 0.7979 - val_loss: 0.4932\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 16: val_loss improved from 0.49323 to 0.49005, saving model to best_model.keras\n",
            "17/17 - 0s - 6ms/step - accuracy: 0.8082 - auc: 0.8694 - loss: 0.4305 - val_accuracy: 0.7304 - val_auc: 0.8011 - val_loss: 0.4900\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.49005\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.7840 - auc: 0.8572 - loss: 0.4411 - val_accuracy: 0.7217 - val_auc: 0.8027 - val_loss: 0.4901\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 18: val_loss improved from 0.49005 to 0.48983, saving model to best_model.keras\n",
            "17/17 - 0s - 11ms/step - accuracy: 0.8007 - auc: 0.8777 - loss: 0.4134 - val_accuracy: 0.7217 - val_auc: 0.8034 - val_loss: 0.4898\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.7877 - auc: 0.8707 - loss: 0.4241 - val_accuracy: 0.7304 - val_auc: 0.8029 - val_loss: 0.4927\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.7896 - auc: 0.8622 - loss: 0.4360 - val_accuracy: 0.7217 - val_auc: 0.8041 - val_loss: 0.4974\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8175 - auc: 0.8834 - loss: 0.4053 - val_accuracy: 0.7304 - val_auc: 0.8068 - val_loss: 0.4928\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 22: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 3ms/step - accuracy: 0.8119 - auc: 0.8807 - loss: 0.4106 - val_accuracy: 0.7478 - val_auc: 0.8089 - val_loss: 0.4907\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8082 - auc: 0.8707 - loss: 0.4256 - val_accuracy: 0.7304 - val_auc: 0.8061 - val_loss: 0.4980\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 24: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 8ms/step - accuracy: 0.8026 - auc: 0.8799 - loss: 0.4056 - val_accuracy: 0.7391 - val_auc: 0.8109 - val_loss: 0.4938\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8212 - auc: 0.8813 - loss: 0.4095 - val_accuracy: 0.7391 - val_auc: 0.8086 - val_loss: 0.4980\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8138 - auc: 0.8796 - loss: 0.4106 - val_accuracy: 0.7304 - val_auc: 0.8102 - val_loss: 0.4959\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8063 - auc: 0.8728 - loss: 0.4278 - val_accuracy: 0.7304 - val_auc: 0.8104 - val_loss: 0.4948\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8119 - auc: 0.8856 - loss: 0.4073 - val_accuracy: 0.7391 - val_auc: 0.8096 - val_loss: 0.4981\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 29: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 4ms/step - accuracy: 0.8194 - auc: 0.8835 - loss: 0.4079 - val_accuracy: 0.7391 - val_auc: 0.8091 - val_loss: 0.4980\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 30: val_loss did not improve from 0.48983\n",
            "17/17 - 0s - 5ms/step - accuracy: 0.8138 - auc: 0.8829 - loss: 0.4045 - val_accuracy: 0.7391 - val_auc: 0.8123 - val_loss: 0.4965\n",
            "Epoch 30: early stopping\n",
            "Restoring model weights from the end of the best epoch: 18.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate pada test set\n",
        "\n",
        "print(\"\\nEvaluasi pada data uji:\")\n",
        "loss, acc, auc = model.evaluate(test_ds, verbose=0)\n",
        "print(f\"Loss: {loss:.4f} | Akurasi: {acc:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "# prediksi probabilitas untuk seluruh test set\n",
        "y_proba = model.predict(test_ds).ravel()\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "cm = tf.math.confusion_matrix(y_test, y_pred, num_classes=2).numpy()\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0 # Dari semua yang dikira “diabetes”, berapa yang benar-benar diabetes?\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0 # Dari semua pasien diabetes, berapa yang berhasil dikenali model?\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0 # Dari semua pasien sehat, berapa yang tidak salah dikira sakit?\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "#pasien sehat -> sehat    pasien sehat -> diabetes\n",
        "#pasien diabetes -> sehat    pasien diabetes -> diabetes\n",
        "print(f\"Precision: {precision:.4f} | Recall (Sensitivitas): {recall:.4f} | Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQuWezw5-HIU",
        "outputId": "49ce94a6-8b48-4e10-cdc3-c821cf6ad493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluasi pada data uji:\n",
            "Loss: 0.4460 | Akurasi: 0.7586 | AUC: 0.8597\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Confusion Matrix:\n",
            " [[65  7]\n",
            " [21 23]]\n",
            "Precision: 0.7667 | Recall (Sensitivitas): 0.5227 | Specificity: 0.9028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Cetak semua prediksi\n",
        "print(\"\\n=== Hasil Prediksi untuk SEMUA Data Test ===\")\n",
        "for i in range(len(X_test)):\n",
        "    p = float(model.predict(X_test[i:i+1])[0,0])\n",
        "    print(f\"Data {i:03d}: Prob={p:.3f} → Prediksi={'Sakit' if p>=0.5 else 'Sehat'} | Label Asli={'Sakit' if y_test[i]==1 else 'Sehat'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ1WVUpx-MtN",
        "outputId": "2ac4fd7b-808c-416a-96b2-45c1fa8a4036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Hasil Prediksi untuk SEMUA Data Test ===\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Data 000: Prob=0.270 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Data 001: Prob=0.542 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 002: Prob=0.037 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 003: Prob=0.003 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 004: Prob=0.138 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 005: Prob=0.007 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 006: Prob=0.014 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 007: Prob=0.265 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 008: Prob=0.644 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 009: Prob=0.731 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Data 010: Prob=0.747 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 011: Prob=0.003 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 012: Prob=0.017 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 013: Prob=0.813 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 014: Prob=0.541 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 015: Prob=0.737 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 016: Prob=0.447 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 017: Prob=0.012 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Data 018: Prob=0.246 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Data 019: Prob=0.810 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Data 020: Prob=0.453 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Data 021: Prob=0.304 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Data 022: Prob=0.013 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Data 023: Prob=0.328 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Data 024: Prob=0.419 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Data 025: Prob=0.009 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Data 026: Prob=0.825 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Data 027: Prob=0.170 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Data 028: Prob=0.381 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Data 029: Prob=0.712 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Data 030: Prob=0.318 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Data 031: Prob=0.008 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Data 032: Prob=0.347 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Data 033: Prob=0.812 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Data 034: Prob=0.222 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 035: Prob=0.016 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 036: Prob=0.856 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 037: Prob=0.163 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 038: Prob=0.348 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 039: Prob=0.436 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 040: Prob=0.244 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 041: Prob=0.313 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 042: Prob=0.122 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 043: Prob=0.350 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 044: Prob=0.240 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 045: Prob=0.014 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 046: Prob=0.261 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 047: Prob=0.079 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 048: Prob=0.091 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 049: Prob=0.867 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 050: Prob=0.008 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 051: Prob=0.137 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 052: Prob=0.441 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 053: Prob=0.055 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 054: Prob=0.833 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 055: Prob=0.220 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Data 056: Prob=0.395 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 057: Prob=0.345 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 058: Prob=0.822 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Data 059: Prob=0.230 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 060: Prob=0.548 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 061: Prob=0.108 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 062: Prob=0.966 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 063: Prob=0.205 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 064: Prob=0.086 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 065: Prob=0.385 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 066: Prob=0.051 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 067: Prob=0.460 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 068: Prob=0.848 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 069: Prob=0.338 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 070: Prob=0.693 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Data 071: Prob=0.724 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 072: Prob=0.284 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Data 073: Prob=0.367 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 074: Prob=0.171 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 075: Prob=0.172 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 076: Prob=0.085 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Data 077: Prob=0.482 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 078: Prob=0.544 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 079: Prob=0.437 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 080: Prob=0.717 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 081: Prob=0.158 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Data 082: Prob=0.763 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Data 083: Prob=0.123 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Data 084: Prob=0.338 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 085: Prob=0.009 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 086: Prob=0.312 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 087: Prob=0.067 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 088: Prob=0.126 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Data 089: Prob=0.467 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Data 090: Prob=0.122 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 091: Prob=0.420 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 092: Prob=0.006 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 093: Prob=0.087 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Data 094: Prob=0.221 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Data 095: Prob=0.418 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 096: Prob=0.486 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 097: Prob=0.981 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Data 098: Prob=0.582 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 099: Prob=0.796 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 100: Prob=0.427 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 101: Prob=0.112 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 102: Prob=0.237 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Data 103: Prob=0.763 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 104: Prob=0.321 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Data 105: Prob=0.444 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 106: Prob=0.003 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 107: Prob=0.224 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 108: Prob=0.724 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 109: Prob=0.492 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 110: Prob=0.436 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Data 111: Prob=0.507 → Prediksi=Sakit | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Data 112: Prob=0.052 → Prediksi=Sehat | Label Asli=Sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Data 113: Prob=0.756 → Prediksi=Sakit | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Data 114: Prob=0.499 → Prediksi=Sehat | Label Asli=Sakit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Data 115: Prob=0.087 → Prediksi=Sehat | Label Asli=Sehat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. Discovering Shopper Groups with TensorFlow - Clustering"
      ],
      "metadata": {
        "id": "sEwpC1WEXvnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tujuan: Analisis perilaku pengeluaran"
      ],
      "metadata": {
        "id": "RhZSiud6iP9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengaturan dasar\n",
        "URL = \"https://raw.githubusercontent.com/sharmaroshan/Clustering-of-Mall-Customers/master/Mall_Customers.csv\"\n",
        "FEATURES = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
        "K = 5 # bagi data jadi 5 cluster\n",
        "MAX_STEPS = 100 # batas iterasi maksimum untuk algoritma iteratif\n",
        "TOL = 1e-4 # perubahan sekecil ini sudah dianggap stabil\n",
        "SEED = 42\n",
        "\n",
        "# Agar hasil reproducible\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "T1LCkxG_ZODs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data & preprocessing\n",
        "# Ambil data langsung dari URL\n",
        "df = pd.read_csv(URL)\n",
        "X = df[FEATURES].to_numpy(dtype=float)\n",
        "\n",
        "# Standarisasi data (agar tiap fitur punya skala sama)\n",
        "# mean = 0 dan std = 1\n",
        "mean = X.mean(axis=0, keepdims=True)\n",
        "std = X.std(axis=0, keepdims=True)\n",
        "std[std == 0] = 1.0\n",
        "X_std = ((X - mean) / std).astype(np.float32)\n",
        "\n",
        "print(\"Data shape:\", X_std.shape)\n",
        "print(\"Fitur yang digunakan:\", FEATURES)\n",
        "\n",
        "print(\"\\n📋 Semua data setelah standarisasi:\")\n",
        "for i, row in enumerate(X_std):\n",
        "    vals = \", \".join([f\"{name}={val:.3f}\" for name, val in zip(FEATURES, row)])\n",
        "    print(f\"{i:03d} | {vals}\")\n",
        "print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "OuMXpxDsR6aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84e45c3-48a2-43b4-97ba-0939bae013b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (200, 3)\n",
            "Fitur yang digunakan: ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
            "\n",
            "📋 Semua data setelah standarisasi:\n",
            "000 | Age=-1.425, Annual Income (k$)=-1.739, Spending Score (1-100)=-0.435\n",
            "001 | Age=-1.281, Annual Income (k$)=-1.739, Spending Score (1-100)=1.196\n",
            "002 | Age=-1.353, Annual Income (k$)=-1.701, Spending Score (1-100)=-1.716\n",
            "003 | Age=-1.138, Annual Income (k$)=-1.701, Spending Score (1-100)=1.040\n",
            "004 | Age=-0.563, Annual Income (k$)=-1.663, Spending Score (1-100)=-0.396\n",
            "005 | Age=-1.209, Annual Income (k$)=-1.663, Spending Score (1-100)=1.002\n",
            "006 | Age=-0.276, Annual Income (k$)=-1.624, Spending Score (1-100)=-1.716\n",
            "007 | Age=-1.138, Annual Income (k$)=-1.624, Spending Score (1-100)=1.700\n",
            "008 | Age=1.805, Annual Income (k$)=-1.586, Spending Score (1-100)=-1.832\n",
            "009 | Age=-0.635, Annual Income (k$)=-1.586, Spending Score (1-100)=0.846\n",
            "010 | Age=2.020, Annual Income (k$)=-1.586, Spending Score (1-100)=-1.405\n",
            "011 | Age=-0.276, Annual Income (k$)=-1.586, Spending Score (1-100)=1.894\n",
            "012 | Age=1.374, Annual Income (k$)=-1.548, Spending Score (1-100)=-1.367\n",
            "013 | Age=-1.066, Annual Income (k$)=-1.548, Spending Score (1-100)=1.040\n",
            "014 | Age=-0.133, Annual Income (k$)=-1.548, Spending Score (1-100)=-1.444\n",
            "015 | Age=-1.209, Annual Income (k$)=-1.548, Spending Score (1-100)=1.118\n",
            "016 | Age=-0.276, Annual Income (k$)=-1.510, Spending Score (1-100)=-0.590\n",
            "017 | Age=-1.353, Annual Income (k$)=-1.510, Spending Score (1-100)=0.613\n",
            "018 | Age=0.944, Annual Income (k$)=-1.434, Spending Score (1-100)=-0.823\n",
            "019 | Age=-0.276, Annual Income (k$)=-1.434, Spending Score (1-100)=1.856\n",
            "020 | Age=-0.276, Annual Income (k$)=-1.395, Spending Score (1-100)=-0.590\n",
            "021 | Age=-0.994, Annual Income (k$)=-1.395, Spending Score (1-100)=0.885\n",
            "022 | Age=0.513, Annual Income (k$)=-1.357, Spending Score (1-100)=-1.755\n",
            "023 | Age=-0.563, Annual Income (k$)=-1.357, Spending Score (1-100)=0.885\n",
            "024 | Age=1.087, Annual Income (k$)=-1.243, Spending Score (1-100)=-1.405\n",
            "025 | Age=-0.707, Annual Income (k$)=-1.243, Spending Score (1-100)=1.235\n",
            "026 | Age=0.441, Annual Income (k$)=-1.243, Spending Score (1-100)=-0.707\n",
            "027 | Age=-0.276, Annual Income (k$)=-1.243, Spending Score (1-100)=0.419\n",
            "028 | Age=0.083, Annual Income (k$)=-1.205, Spending Score (1-100)=-0.745\n",
            "029 | Age=-1.138, Annual Income (k$)=-1.205, Spending Score (1-100)=1.429\n",
            "030 | Age=1.518, Annual Income (k$)=-1.166, Spending Score (1-100)=-1.794\n",
            "031 | Age=-1.281, Annual Income (k$)=-1.166, Spending Score (1-100)=0.885\n",
            "032 | Age=1.015, Annual Income (k$)=-1.052, Spending Score (1-100)=-1.794\n",
            "033 | Age=-1.496, Annual Income (k$)=-1.052, Spending Score (1-100)=1.623\n",
            "034 | Age=0.728, Annual Income (k$)=-1.052, Spending Score (1-100)=-1.405\n",
            "035 | Age=-1.281, Annual Income (k$)=-1.052, Spending Score (1-100)=1.196\n",
            "036 | Age=0.226, Annual Income (k$)=-1.014, Spending Score (1-100)=-1.289\n",
            "037 | Age=-0.635, Annual Income (k$)=-1.014, Spending Score (1-100)=0.885\n",
            "038 | Age=-0.205, Annual Income (k$)=-0.899, Spending Score (1-100)=-0.939\n",
            "039 | Age=-1.353, Annual Income (k$)=-0.899, Spending Score (1-100)=0.963\n",
            "040 | Age=1.877, Annual Income (k$)=-0.861, Spending Score (1-100)=-0.590\n",
            "041 | Age=-1.066, Annual Income (k$)=-0.861, Spending Score (1-100)=1.623\n",
            "042 | Age=0.657, Annual Income (k$)=-0.823, Spending Score (1-100)=-0.551\n",
            "043 | Age=-0.563, Annual Income (k$)=-0.823, Spending Score (1-100)=0.419\n",
            "044 | Age=0.728, Annual Income (k$)=-0.823, Spending Score (1-100)=-0.862\n",
            "045 | Age=-1.066, Annual Income (k$)=-0.823, Spending Score (1-100)=0.575\n",
            "046 | Age=0.800, Annual Income (k$)=-0.785, Spending Score (1-100)=0.186\n",
            "047 | Age=-0.850, Annual Income (k$)=-0.785, Spending Score (1-100)=-0.124\n",
            "048 | Age=-0.707, Annual Income (k$)=-0.785, Spending Score (1-100)=-0.318\n",
            "049 | Age=-0.563, Annual Income (k$)=-0.785, Spending Score (1-100)=-0.318\n",
            "050 | Age=0.728, Annual Income (k$)=-0.708, Spending Score (1-100)=0.070\n",
            "051 | Age=-0.420, Annual Income (k$)=-0.708, Spending Score (1-100)=0.380\n",
            "052 | Age=-0.563, Annual Income (k$)=-0.670, Spending Score (1-100)=0.148\n",
            "053 | Age=1.446, Annual Income (k$)=-0.670, Spending Score (1-100)=0.380\n",
            "054 | Age=0.800, Annual Income (k$)=-0.670, Spending Score (1-100)=-0.202\n",
            "055 | Age=0.585, Annual Income (k$)=-0.670, Spending Score (1-100)=-0.357\n",
            "056 | Age=0.872, Annual Income (k$)=-0.632, Spending Score (1-100)=-0.008\n",
            "057 | Age=2.164, Annual Income (k$)=-0.632, Spending Score (1-100)=-0.163\n",
            "058 | Age=-0.850, Annual Income (k$)=-0.556, Spending Score (1-100)=0.031\n",
            "059 | Age=1.015, Annual Income (k$)=-0.556, Spending Score (1-100)=-0.163\n",
            "060 | Age=2.236, Annual Income (k$)=-0.556, Spending Score (1-100)=0.225\n",
            "061 | Age=-1.425, Annual Income (k$)=-0.556, Spending Score (1-100)=0.186\n",
            "062 | Age=2.020, Annual Income (k$)=-0.518, Spending Score (1-100)=0.070\n",
            "063 | Age=1.087, Annual Income (k$)=-0.518, Spending Score (1-100)=0.342\n",
            "064 | Age=1.733, Annual Income (k$)=-0.479, Spending Score (1-100)=0.031\n",
            "065 | Age=-1.496, Annual Income (k$)=-0.479, Spending Score (1-100)=0.342\n",
            "066 | Age=0.298, Annual Income (k$)=-0.479, Spending Score (1-100)=-0.008\n",
            "067 | Age=2.092, Annual Income (k$)=-0.479, Spending Score (1-100)=-0.085\n",
            "068 | Age=-1.425, Annual Income (k$)=-0.479, Spending Score (1-100)=0.342\n",
            "069 | Age=-0.492, Annual Income (k$)=-0.479, Spending Score (1-100)=-0.124\n",
            "070 | Age=2.236, Annual Income (k$)=-0.441, Spending Score (1-100)=0.186\n",
            "071 | Age=0.585, Annual Income (k$)=-0.441, Spending Score (1-100)=-0.318\n",
            "072 | Age=1.518, Annual Income (k$)=-0.403, Spending Score (1-100)=-0.047\n",
            "073 | Age=1.518, Annual Income (k$)=-0.403, Spending Score (1-100)=0.225\n",
            "074 | Age=1.446, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.124\n",
            "075 | Age=-0.922, Annual Income (k$)=-0.250, Spending Score (1-100)=0.148\n",
            "076 | Age=0.441, Annual Income (k$)=-0.250, Spending Score (1-100)=0.109\n",
            "077 | Age=0.083, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.085\n",
            "078 | Age=-1.138, Annual Income (k$)=-0.250, Spending Score (1-100)=0.070\n",
            "079 | Age=0.728, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.318\n",
            "080 | Age=1.303, Annual Income (k$)=-0.250, Spending Score (1-100)=0.031\n",
            "081 | Age=-0.061, Annual Income (k$)=-0.250, Spending Score (1-100)=0.186\n",
            "082 | Age=2.020, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.357\n",
            "083 | Age=0.513, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.241\n",
            "084 | Age=-1.281, Annual Income (k$)=-0.250, Spending Score (1-100)=0.264\n",
            "085 | Age=0.657, Annual Income (k$)=-0.250, Spending Score (1-100)=-0.163\n",
            "086 | Age=1.159, Annual Income (k$)=-0.136, Spending Score (1-100)=0.303\n",
            "087 | Age=-1.209, Annual Income (k$)=-0.136, Spending Score (1-100)=0.186\n",
            "088 | Age=-0.348, Annual Income (k$)=-0.098, Spending Score (1-100)=0.380\n",
            "089 | Age=0.800, Annual Income (k$)=-0.098, Spending Score (1-100)=-0.163\n",
            "090 | Age=2.092, Annual Income (k$)=-0.060, Spending Score (1-100)=0.186\n",
            "091 | Age=-1.496, Annual Income (k$)=-0.060, Spending Score (1-100)=-0.357\n",
            "092 | Age=0.657, Annual Income (k$)=-0.021, Spending Score (1-100)=-0.047\n",
            "093 | Age=0.083, Annual Income (k$)=-0.021, Spending Score (1-100)=-0.396\n",
            "094 | Age=-0.492, Annual Income (k$)=-0.021, Spending Score (1-100)=-0.318\n",
            "095 | Age=-1.066, Annual Income (k$)=-0.021, Spending Score (1-100)=0.070\n",
            "096 | Age=0.585, Annual Income (k$)=-0.021, Spending Score (1-100)=-0.124\n",
            "097 | Age=-0.850, Annual Income (k$)=-0.021, Spending Score (1-100)=-0.008\n",
            "098 | Age=0.657, Annual Income (k$)=0.017, Spending Score (1-100)=-0.318\n",
            "099 | Age=-1.353, Annual Income (k$)=0.017, Spending Score (1-100)=-0.047\n",
            "100 | Age=-1.138, Annual Income (k$)=0.055, Spending Score (1-100)=-0.357\n",
            "101 | Age=0.728, Annual Income (k$)=0.055, Spending Score (1-100)=-0.085\n",
            "102 | Age=2.020, Annual Income (k$)=0.055, Spending Score (1-100)=0.342\n",
            "103 | Age=-0.922, Annual Income (k$)=0.055, Spending Score (1-100)=0.186\n",
            "104 | Age=0.728, Annual Income (k$)=0.055, Spending Score (1-100)=0.225\n",
            "105 | Age=-1.281, Annual Income (k$)=0.055, Spending Score (1-100)=-0.318\n",
            "106 | Age=1.948, Annual Income (k$)=0.093, Spending Score (1-100)=-0.008\n",
            "107 | Age=1.087, Annual Income (k$)=0.093, Spending Score (1-100)=-0.163\n",
            "108 | Age=2.092, Annual Income (k$)=0.093, Spending Score (1-100)=-0.280\n",
            "109 | Age=1.948, Annual Income (k$)=0.093, Spending Score (1-100)=-0.085\n",
            "110 | Age=1.877, Annual Income (k$)=0.093, Spending Score (1-100)=0.070\n",
            "111 | Age=-1.425, Annual Income (k$)=0.093, Spending Score (1-100)=0.148\n",
            "112 | Age=-0.061, Annual Income (k$)=0.131, Spending Score (1-100)=-0.318\n",
            "113 | Age=-1.425, Annual Income (k$)=0.131, Spending Score (1-100)=-0.163\n",
            "114 | Age=-1.496, Annual Income (k$)=0.169, Spending Score (1-100)=-0.085\n",
            "115 | Age=-1.425, Annual Income (k$)=0.169, Spending Score (1-100)=-0.008\n",
            "116 | Age=1.733, Annual Income (k$)=0.169, Spending Score (1-100)=-0.280\n",
            "117 | Age=0.728, Annual Income (k$)=0.169, Spending Score (1-100)=0.342\n",
            "118 | Age=0.872, Annual Income (k$)=0.246, Spending Score (1-100)=-0.280\n",
            "119 | Age=0.800, Annual Income (k$)=0.246, Spending Score (1-100)=0.264\n",
            "120 | Age=-0.850, Annual Income (k$)=0.246, Spending Score (1-100)=0.225\n",
            "121 | Age=-0.061, Annual Income (k$)=0.246, Spending Score (1-100)=-0.396\n",
            "122 | Age=0.083, Annual Income (k$)=0.322, Spending Score (1-100)=0.303\n",
            "123 | Age=0.011, Annual Income (k$)=0.322, Spending Score (1-100)=1.584\n",
            "124 | Age=-1.138, Annual Income (k$)=0.360, Spending Score (1-100)=-0.823\n",
            "125 | Age=-0.563, Annual Income (k$)=0.360, Spending Score (1-100)=1.040\n",
            "126 | Age=0.298, Annual Income (k$)=0.398, Spending Score (1-100)=-0.590\n",
            "127 | Age=0.083, Annual Income (k$)=0.398, Spending Score (1-100)=1.739\n",
            "128 | Age=1.446, Annual Income (k$)=0.398, Spending Score (1-100)=-1.522\n",
            "129 | Age=-0.061, Annual Income (k$)=0.398, Spending Score (1-100)=0.963\n",
            "130 | Age=0.585, Annual Income (k$)=0.398, Spending Score (1-100)=-1.599\n",
            "131 | Age=0.011, Annual Income (k$)=0.398, Spending Score (1-100)=0.963\n",
            "132 | Age=-0.994, Annual Income (k$)=0.437, Spending Score (1-100)=-0.629\n",
            "133 | Age=-0.563, Annual Income (k$)=0.437, Spending Score (1-100)=0.807\n",
            "134 | Age=-1.353, Annual Income (k$)=0.475, Spending Score (1-100)=-1.755\n",
            "135 | Age=-0.707, Annual Income (k$)=0.475, Spending Score (1-100)=1.467\n",
            "136 | Age=0.370, Annual Income (k$)=0.475, Spending Score (1-100)=-1.677\n",
            "137 | Age=-0.492, Annual Income (k$)=0.475, Spending Score (1-100)=0.885\n",
            "138 | Age=-1.425, Annual Income (k$)=0.513, Spending Score (1-100)=-1.561\n",
            "139 | Age=-0.276, Annual Income (k$)=0.513, Spending Score (1-100)=0.846\n",
            "140 | Age=1.303, Annual Income (k$)=0.551, Spending Score (1-100)=-1.755\n",
            "141 | Age=-0.492, Annual Income (k$)=0.551, Spending Score (1-100)=1.662\n",
            "142 | Age=-0.779, Annual Income (k$)=0.589, Spending Score (1-100)=-0.396\n",
            "143 | Age=-0.492, Annual Income (k$)=0.589, Spending Score (1-100)=1.429\n",
            "144 | Age=-0.994, Annual Income (k$)=0.628, Spending Score (1-100)=-1.483\n",
            "145 | Age=-0.779, Annual Income (k$)=0.628, Spending Score (1-100)=1.817\n",
            "146 | Age=0.657, Annual Income (k$)=0.628, Spending Score (1-100)=-0.551\n",
            "147 | Age=-0.492, Annual Income (k$)=0.628, Spending Score (1-100)=0.924\n",
            "148 | Age=-0.348, Annual Income (k$)=0.666, Spending Score (1-100)=-1.095\n",
            "149 | Age=-0.348, Annual Income (k$)=0.666, Spending Score (1-100)=1.545\n",
            "150 | Age=0.298, Annual Income (k$)=0.666, Spending Score (1-100)=-1.289\n",
            "151 | Age=0.011, Annual Income (k$)=0.666, Spending Score (1-100)=1.467\n",
            "152 | Age=0.370, Annual Income (k$)=0.666, Spending Score (1-100)=-1.172\n",
            "153 | Age=-0.061, Annual Income (k$)=0.666, Spending Score (1-100)=1.002\n",
            "154 | Age=0.585, Annual Income (k$)=0.666, Spending Score (1-100)=-1.328\n",
            "155 | Age=-0.850, Annual Income (k$)=0.666, Spending Score (1-100)=1.506\n",
            "156 | Age=-0.133, Annual Income (k$)=0.666, Spending Score (1-100)=-1.910\n",
            "157 | Age=-0.635, Annual Income (k$)=0.666, Spending Score (1-100)=1.079\n",
            "158 | Age=-0.348, Annual Income (k$)=0.666, Spending Score (1-100)=-1.910\n",
            "159 | Age=-0.635, Annual Income (k$)=0.666, Spending Score (1-100)=0.885\n",
            "160 | Age=1.231, Annual Income (k$)=0.704, Spending Score (1-100)=-0.590\n",
            "161 | Age=-0.707, Annual Income (k$)=0.704, Spending Score (1-100)=1.273\n",
            "162 | Age=-1.425, Annual Income (k$)=0.780, Spending Score (1-100)=-1.755\n",
            "163 | Age=-0.563, Annual Income (k$)=0.780, Spending Score (1-100)=1.662\n",
            "164 | Age=0.800, Annual Income (k$)=0.933, Spending Score (1-100)=-0.939\n",
            "165 | Age=-0.205, Annual Income (k$)=0.933, Spending Score (1-100)=0.963\n",
            "166 | Age=0.226, Annual Income (k$)=0.971, Spending Score (1-100)=-1.172\n",
            "167 | Age=-0.420, Annual Income (k$)=0.971, Spending Score (1-100)=1.739\n",
            "168 | Age=-0.205, Annual Income (k$)=1.009, Spending Score (1-100)=-0.901\n",
            "169 | Age=-0.492, Annual Income (k$)=1.009, Spending Score (1-100)=0.497\n",
            "170 | Age=0.083, Annual Income (k$)=1.009, Spending Score (1-100)=-1.444\n",
            "171 | Age=-0.779, Annual Income (k$)=1.009, Spending Score (1-100)=0.963\n",
            "172 | Age=-0.205, Annual Income (k$)=1.009, Spending Score (1-100)=-1.561\n",
            "173 | Age=-0.205, Annual Income (k$)=1.009, Spending Score (1-100)=1.623\n",
            "174 | Age=0.944, Annual Income (k$)=1.047, Spending Score (1-100)=-1.444\n",
            "175 | Age=-0.635, Annual Income (k$)=1.047, Spending Score (1-100)=1.390\n",
            "176 | Age=1.374, Annual Income (k$)=1.047, Spending Score (1-100)=-1.367\n",
            "177 | Age=-0.850, Annual Income (k$)=1.047, Spending Score (1-100)=0.730\n",
            "178 | Age=1.446, Annual Income (k$)=1.238, Spending Score (1-100)=-1.405\n",
            "179 | Age=-0.276, Annual Income (k$)=1.238, Spending Score (1-100)=1.545\n",
            "180 | Age=-0.133, Annual Income (k$)=1.391, Spending Score (1-100)=-0.707\n",
            "181 | Age=-0.492, Annual Income (k$)=1.391, Spending Score (1-100)=1.390\n",
            "182 | Age=0.513, Annual Income (k$)=1.429, Spending Score (1-100)=-1.367\n",
            "183 | Age=-0.707, Annual Income (k$)=1.429, Spending Score (1-100)=1.467\n",
            "184 | Age=0.154, Annual Income (k$)=1.467, Spending Score (1-100)=-0.435\n",
            "185 | Age=-0.635, Annual Income (k$)=1.467, Spending Score (1-100)=1.817\n",
            "186 | Age=1.087, Annual Income (k$)=1.544, Spending Score (1-100)=-1.017\n",
            "187 | Age=-0.779, Annual Income (k$)=1.544, Spending Score (1-100)=0.691\n",
            "188 | Age=0.154, Annual Income (k$)=1.620, Spending Score (1-100)=-1.289\n",
            "189 | Age=-0.205, Annual Income (k$)=1.620, Spending Score (1-100)=1.351\n",
            "190 | Age=-0.348, Annual Income (k$)=1.620, Spending Score (1-100)=-1.056\n",
            "191 | Age=-0.492, Annual Income (k$)=1.620, Spending Score (1-100)=0.730\n",
            "192 | Age=-0.420, Annual Income (k$)=2.002, Spending Score (1-100)=-1.638\n",
            "193 | Age=-0.061, Annual Income (k$)=2.002, Spending Score (1-100)=1.584\n",
            "194 | Age=0.585, Annual Income (k$)=2.269, Spending Score (1-100)=-1.328\n",
            "195 | Age=-0.276, Annual Income (k$)=2.269, Spending Score (1-100)=1.118\n",
            "196 | Age=0.441, Annual Income (k$)=2.498, Spending Score (1-100)=-0.862\n",
            "197 | Age=-0.492, Annual Income (k$)=2.498, Spending Score (1-100)=0.924\n",
            "198 | Age=-0.492, Annual Income (k$)=2.918, Spending Score (1-100)=-1.250\n",
            "199 | Age=-0.635, Annual Income (k$)=2.918, Spending Score (1-100)=1.273\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Implementasi KMeans dengan TensorFlow\n",
        "def tf_kmeans(X_np, k, max_steps=100, tol=1e-4, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X_tf = tf.constant(X_np, dtype=tf.float32)\n",
        "    n = X_np.shape[0]\n",
        "\n",
        "    # Inisialisasi centroid acak\n",
        "    init_idx = rng.choice(n, size=k, replace=False)\n",
        "    centroids = tf.Variable(tf.gather(X_tf, init_idx), dtype=tf.float32)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # --- Step 1: Assign (hitung jarak ke setiap centroid) ---\n",
        "        # nambah dimensi spy bisa pakai broadcasting\n",
        "        expanded_x = tf.expand_dims(X_tf, 1)      # [n, 1, d]\n",
        "        expanded_c = tf.expand_dims(centroids, 0) # [1, k, d]\n",
        "        # hitung jarak kuadrat euclidean antara tiap titik dan tiap sentroid\n",
        "        dists = tf.reduce_sum(tf.square(expanded_x - expanded_c), axis=2)  # [n, k]\n",
        "        labels = tf.argmin(dists, axis=1, output_type=tf.int32)\n",
        "        # untuk tiap titik, ambil indeks centroid dengan jarak paling kecil\n",
        "\n",
        "        # --- Step 2: Update (rata-rata titik per cluster) ---\n",
        "        new_centroids = []\n",
        "        for j in range(k):\n",
        "            mask = tf.equal(labels, j)\n",
        "            count = tf.reduce_sum(tf.cast(mask, tf.int32)) # jumlah titik yg masuk cluster j\n",
        "            def mean_cluster(): #ambil semua titik di cluster j dan hitung rata-rata\n",
        "                pts = tf.boolean_mask(X_tf, mask)\n",
        "                return tf.reduce_mean(pts, axis=0)\n",
        "            def reinit(): # pilih titik acak dari seluruh data untuk jadi centroid baru jika cluster kosong\n",
        "                return X_tf[int(rng.randint(0, n))]\n",
        "            c_j = tf.cond(count > 0, mean_cluster, reinit)\n",
        "            new_centroids.append(c_j)\n",
        "        new_centroids = tf.stack(new_centroids, axis=0)\n",
        "\n",
        "        # --- Step 3: Check pergeseran centroid ---\n",
        "        shift = tf.reduce_max(tf.norm(new_centroids - centroids, axis=1))\n",
        "        centroids.assign(new_centroids)\n",
        "        if shift < tol:\n",
        "            print(f\"Converged at step {step+1}, shift={shift.numpy():.6f}\")\n",
        "            break\n",
        "\n",
        "    # Final label assignment\n",
        "    expanded_x = tf.expand_dims(X_tf, 1)\n",
        "    expanded_c = tf.expand_dims(centroids, 0)\n",
        "    dists = tf.reduce_sum(tf.square(expanded_x - expanded_c), axis=2)\n",
        "    labels = tf.argmin(dists, axis=1, output_type=tf.int32)\n",
        "    return labels.numpy(), centroids.numpy()\n"
      ],
      "metadata": {
        "id": "0awXl0CXVBzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Jalankan Clustering TensorFlow\n",
        "labels, centroids = tf_kmeans(X_std, K, max_steps=MAX_STEPS, tol=TOL, seed=SEED)\n"
      ],
      "metadata": {
        "id": "szLYg8uCZaWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f558d4-1656-4080-b6e9-c0b79d375a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at step 9, shift=0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Evaluasi dan hasil\n",
        "print(\"\\n=== HASIL CLUSTERING ===\")\n",
        "\n",
        "# a) Ukuran cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"Cluster {u}: {c} data\")\n",
        "\n",
        "# b) Centroid standar & skala asli\n",
        "centroids_orig = (centroids * std) + mean\n",
        "print(\"\\nCentroid dalam ruang standar:\")\n",
        "print(np.round(centroids, 3))\n",
        "print(\"\\nCentroid dalam skala asli:\")\n",
        "for i, c in enumerate(centroids_orig):\n",
        "    vals = \", \".join([f\"{name}={val:.2f}\" for name, val in zip(FEATURES, c)])\n",
        "    print(f\"Cluster {i}: {vals}\")\n",
        "\n",
        "# c) Inertia (total jarak kuadrat ke centroid)\n",
        "def inertia(X, labels, centroids):\n",
        "    s = 0.0\n",
        "    for i, c in enumerate(centroids):\n",
        "        mask = labels == i\n",
        "        if mask.sum() == 0: continue\n",
        "        dif = X[mask] - c[None, :]\n",
        "        s += (dif**2).sum()\n",
        "    return s\n",
        "inertia_val = inertia(X_std, labels, centroids)\n",
        "print(f\"\\nInertia (total distance): {inertia_val:.4f}\")\n",
        "\n",
        "# d) Representative sample\n",
        "print(\"\\nRepresentative sample per cluster (terdekat dengan centroid):\")\n",
        "for i in range(K):\n",
        "    mask = labels == i\n",
        "    if mask.sum() == 0:\n",
        "        print(f\"Cluster {i}: (kosong)\")\n",
        "        continue\n",
        "    dists = np.linalg.norm(X_std - centroids[i][None, :], axis=1)\n",
        "    idx = np.argmin(dists)\n",
        "    row = df.iloc[idx]\n",
        "    print(f\"Cluster {i}: idx={idx}, Age={row['Age']}, Income={row['Annual Income (k$)']}, Spending={row['Spending Score (1-100)']}\")\n",
        "\n",
        "# e) Semua data + cluster hasilnya\n",
        "print(\"\\n=== Semua Data dengan Label Cluster ===\")\n",
        "print(\"idx | \" + \" | \".join(FEATURES) + \" | Cluster\")\n",
        "for i, row in df.iterrows():\n",
        "    vals = \" | \".join([str(row[f]) for f in FEATURES])\n",
        "    print(f\"{i:03d} | {vals} | {labels[i]}\")"
      ],
      "metadata": {
        "id": "2UWZyqTSZgnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d531bde3-2441-4c07-c70c-4ea2ee3e44d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== HASIL CLUSTERING ===\n",
            "Cluster 0: 40 data\n",
            "Cluster 1: 54 data\n",
            "Cluster 2: 20 data\n",
            "Cluster 3: 39 data\n",
            "Cluster 4: 47 data\n",
            "\n",
            "Centroid dalam ruang standar:\n",
            "[[-0.429  0.975  1.216]\n",
            " [-0.981 -0.743  0.467]\n",
            " [ 0.531 -1.291 -1.236]\n",
            " [ 0.073  0.975 -1.197]\n",
            " [ 1.205 -0.236 -0.052]]\n",
            "\n",
            "Centroid dalam skala asli:\n",
            "Cluster 0: Age=32.87, Annual Income (k$)=86.10, Spending Score (1-100)=81.53\n",
            "Cluster 1: Age=25.19, Annual Income (k$)=41.09, Spending Score (1-100)=62.24\n",
            "Cluster 2: Age=46.25, Annual Income (k$)=26.75, Spending Score (1-100)=18.35\n",
            "Cluster 3: Age=39.87, Annual Income (k$)=86.10, Spending Score (1-100)=19.36\n",
            "Cluster 4: Age=55.64, Annual Income (k$)=54.38, Spending Score (1-100)=48.85\n",
            "\n",
            "Inertia (total distance): 168.2476\n",
            "\n",
            "Representative sample per cluster (terdekat dengan centroid):\n",
            "Cluster 0: idx=175, Age=30, Income=88, Spending=86\n",
            "Cluster 1: idx=45, Age=24, Income=39, Spending=65\n",
            "Cluster 2: idx=34, Age=49, Income=33, Spending=14\n",
            "Cluster 3: idx=166, Age=42, Income=86, Spending=20\n",
            "Cluster 4: idx=80, Age=57, Income=54, Spending=51\n",
            "\n",
            "=== Semua Data dengan Label Cluster ===\n",
            "idx | Age | Annual Income (k$) | Spending Score (1-100) | Cluster\n",
            "000 | 19 | 15 | 39 | 1\n",
            "001 | 21 | 15 | 81 | 1\n",
            "002 | 20 | 16 | 6 | 2\n",
            "003 | 23 | 16 | 77 | 1\n",
            "004 | 31 | 17 | 40 | 1\n",
            "005 | 22 | 17 | 76 | 1\n",
            "006 | 35 | 18 | 6 | 2\n",
            "007 | 23 | 18 | 94 | 1\n",
            "008 | 64 | 19 | 3 | 2\n",
            "009 | 30 | 19 | 72 | 1\n",
            "010 | 67 | 19 | 14 | 2\n",
            "011 | 35 | 19 | 99 | 1\n",
            "012 | 58 | 20 | 15 | 2\n",
            "013 | 24 | 20 | 77 | 1\n",
            "014 | 37 | 20 | 13 | 2\n",
            "015 | 22 | 20 | 79 | 1\n",
            "016 | 35 | 21 | 35 | 2\n",
            "017 | 20 | 21 | 66 | 1\n",
            "018 | 52 | 23 | 29 | 2\n",
            "019 | 35 | 23 | 98 | 1\n",
            "020 | 35 | 24 | 35 | 2\n",
            "021 | 25 | 24 | 73 | 1\n",
            "022 | 46 | 25 | 5 | 2\n",
            "023 | 31 | 25 | 73 | 1\n",
            "024 | 54 | 28 | 14 | 2\n",
            "025 | 29 | 28 | 82 | 1\n",
            "026 | 45 | 28 | 32 | 2\n",
            "027 | 35 | 28 | 61 | 1\n",
            "028 | 40 | 29 | 31 | 2\n",
            "029 | 23 | 29 | 87 | 1\n",
            "030 | 60 | 30 | 4 | 2\n",
            "031 | 21 | 30 | 73 | 1\n",
            "032 | 53 | 33 | 4 | 2\n",
            "033 | 18 | 33 | 92 | 1\n",
            "034 | 49 | 33 | 14 | 2\n",
            "035 | 21 | 33 | 81 | 1\n",
            "036 | 42 | 34 | 17 | 2\n",
            "037 | 30 | 34 | 73 | 1\n",
            "038 | 36 | 37 | 26 | 2\n",
            "039 | 20 | 37 | 75 | 1\n",
            "040 | 65 | 38 | 35 | 4\n",
            "041 | 24 | 38 | 92 | 1\n",
            "042 | 48 | 39 | 36 | 2\n",
            "043 | 31 | 39 | 61 | 1\n",
            "044 | 49 | 39 | 28 | 2\n",
            "045 | 24 | 39 | 65 | 1\n",
            "046 | 50 | 40 | 55 | 4\n",
            "047 | 27 | 40 | 47 | 1\n",
            "048 | 29 | 40 | 42 | 1\n",
            "049 | 31 | 40 | 42 | 1\n",
            "050 | 49 | 42 | 52 | 4\n",
            "051 | 33 | 42 | 60 | 1\n",
            "052 | 31 | 43 | 54 | 1\n",
            "053 | 59 | 43 | 60 | 4\n",
            "054 | 50 | 43 | 45 | 4\n",
            "055 | 47 | 43 | 41 | 4\n",
            "056 | 51 | 44 | 50 | 4\n",
            "057 | 69 | 44 | 46 | 4\n",
            "058 | 27 | 46 | 51 | 1\n",
            "059 | 53 | 46 | 46 | 4\n",
            "060 | 70 | 46 | 56 | 4\n",
            "061 | 19 | 46 | 55 | 1\n",
            "062 | 67 | 47 | 52 | 4\n",
            "063 | 54 | 47 | 59 | 4\n",
            "064 | 63 | 48 | 51 | 4\n",
            "065 | 18 | 48 | 59 | 1\n",
            "066 | 43 | 48 | 50 | 4\n",
            "067 | 68 | 48 | 48 | 4\n",
            "068 | 19 | 48 | 59 | 1\n",
            "069 | 32 | 48 | 47 | 1\n",
            "070 | 70 | 49 | 55 | 4\n",
            "071 | 47 | 49 | 42 | 4\n",
            "072 | 60 | 50 | 49 | 4\n",
            "073 | 60 | 50 | 56 | 4\n",
            "074 | 59 | 54 | 47 | 4\n",
            "075 | 26 | 54 | 54 | 1\n",
            "076 | 45 | 54 | 53 | 4\n",
            "077 | 40 | 54 | 48 | 4\n",
            "078 | 23 | 54 | 52 | 1\n",
            "079 | 49 | 54 | 42 | 4\n",
            "080 | 57 | 54 | 51 | 4\n",
            "081 | 38 | 54 | 55 | 1\n",
            "082 | 67 | 54 | 41 | 4\n",
            "083 | 46 | 54 | 44 | 4\n",
            "084 | 21 | 54 | 57 | 1\n",
            "085 | 48 | 54 | 46 | 4\n",
            "086 | 55 | 57 | 58 | 4\n",
            "087 | 22 | 57 | 55 | 1\n",
            "088 | 34 | 58 | 60 | 1\n",
            "089 | 50 | 58 | 46 | 4\n",
            "090 | 68 | 59 | 55 | 4\n",
            "091 | 18 | 59 | 41 | 1\n",
            "092 | 48 | 60 | 49 | 4\n",
            "093 | 40 | 60 | 40 | 4\n",
            "094 | 32 | 60 | 42 | 1\n",
            "095 | 24 | 60 | 52 | 1\n",
            "096 | 47 | 60 | 47 | 4\n",
            "097 | 27 | 60 | 50 | 1\n",
            "098 | 48 | 61 | 42 | 4\n",
            "099 | 20 | 61 | 49 | 1\n",
            "100 | 23 | 62 | 41 | 1\n",
            "101 | 49 | 62 | 48 | 4\n",
            "102 | 67 | 62 | 59 | 4\n",
            "103 | 26 | 62 | 55 | 1\n",
            "104 | 49 | 62 | 56 | 4\n",
            "105 | 21 | 62 | 42 | 1\n",
            "106 | 66 | 63 | 50 | 4\n",
            "107 | 54 | 63 | 46 | 4\n",
            "108 | 68 | 63 | 43 | 4\n",
            "109 | 66 | 63 | 48 | 4\n",
            "110 | 65 | 63 | 52 | 4\n",
            "111 | 19 | 63 | 54 | 1\n",
            "112 | 38 | 64 | 42 | 3\n",
            "113 | 19 | 64 | 46 | 1\n",
            "114 | 18 | 65 | 48 | 1\n",
            "115 | 19 | 65 | 50 | 1\n",
            "116 | 63 | 65 | 43 | 4\n",
            "117 | 49 | 65 | 59 | 4\n",
            "118 | 51 | 67 | 43 | 4\n",
            "119 | 50 | 67 | 57 | 4\n",
            "120 | 27 | 67 | 56 | 1\n",
            "121 | 38 | 67 | 40 | 3\n",
            "122 | 40 | 69 | 58 | 0\n",
            "123 | 39 | 69 | 91 | 0\n",
            "124 | 23 | 70 | 29 | 3\n",
            "125 | 31 | 70 | 77 | 0\n",
            "126 | 43 | 71 | 35 | 3\n",
            "127 | 40 | 71 | 95 | 0\n",
            "128 | 59 | 71 | 11 | 3\n",
            "129 | 38 | 71 | 75 | 0\n",
            "130 | 47 | 71 | 9 | 3\n",
            "131 | 39 | 71 | 75 | 0\n",
            "132 | 25 | 72 | 34 | 3\n",
            "133 | 31 | 72 | 71 | 0\n",
            "134 | 20 | 73 | 5 | 3\n",
            "135 | 29 | 73 | 88 | 0\n",
            "136 | 44 | 73 | 7 | 3\n",
            "137 | 32 | 73 | 73 | 0\n",
            "138 | 19 | 74 | 10 | 3\n",
            "139 | 35 | 74 | 72 | 0\n",
            "140 | 57 | 75 | 5 | 3\n",
            "141 | 32 | 75 | 93 | 0\n",
            "142 | 28 | 76 | 40 | 3\n",
            "143 | 32 | 76 | 87 | 0\n",
            "144 | 25 | 77 | 12 | 3\n",
            "145 | 28 | 77 | 97 | 0\n",
            "146 | 48 | 77 | 36 | 3\n",
            "147 | 32 | 77 | 74 | 0\n",
            "148 | 34 | 78 | 22 | 3\n",
            "149 | 34 | 78 | 90 | 0\n",
            "150 | 43 | 78 | 17 | 3\n",
            "151 | 39 | 78 | 88 | 0\n",
            "152 | 44 | 78 | 20 | 3\n",
            "153 | 38 | 78 | 76 | 0\n",
            "154 | 47 | 78 | 16 | 3\n",
            "155 | 27 | 78 | 89 | 0\n",
            "156 | 37 | 78 | 1 | 3\n",
            "157 | 30 | 78 | 78 | 0\n",
            "158 | 34 | 78 | 1 | 3\n",
            "159 | 30 | 78 | 73 | 0\n",
            "160 | 56 | 79 | 35 | 4\n",
            "161 | 29 | 79 | 83 | 0\n",
            "162 | 19 | 81 | 5 | 3\n",
            "163 | 31 | 81 | 93 | 0\n",
            "164 | 50 | 85 | 26 | 3\n",
            "165 | 36 | 85 | 75 | 0\n",
            "166 | 42 | 86 | 20 | 3\n",
            "167 | 33 | 86 | 95 | 0\n",
            "168 | 36 | 87 | 27 | 3\n",
            "169 | 32 | 87 | 63 | 0\n",
            "170 | 40 | 87 | 13 | 3\n",
            "171 | 28 | 87 | 75 | 0\n",
            "172 | 36 | 87 | 10 | 3\n",
            "173 | 36 | 87 | 92 | 0\n",
            "174 | 52 | 88 | 13 | 3\n",
            "175 | 30 | 88 | 86 | 0\n",
            "176 | 58 | 88 | 15 | 3\n",
            "177 | 27 | 88 | 69 | 0\n",
            "178 | 59 | 93 | 14 | 3\n",
            "179 | 35 | 93 | 90 | 0\n",
            "180 | 37 | 97 | 32 | 3\n",
            "181 | 32 | 97 | 86 | 0\n",
            "182 | 46 | 98 | 15 | 3\n",
            "183 | 29 | 98 | 88 | 0\n",
            "184 | 41 | 99 | 39 | 3\n",
            "185 | 30 | 99 | 97 | 0\n",
            "186 | 54 | 101 | 24 | 3\n",
            "187 | 28 | 101 | 68 | 0\n",
            "188 | 41 | 103 | 17 | 3\n",
            "189 | 36 | 103 | 85 | 0\n",
            "190 | 34 | 103 | 23 | 3\n",
            "191 | 32 | 103 | 69 | 0\n",
            "192 | 33 | 113 | 8 | 3\n",
            "193 | 38 | 113 | 91 | 0\n",
            "194 | 47 | 120 | 16 | 3\n",
            "195 | 35 | 120 | 79 | 0\n",
            "196 | 45 | 126 | 28 | 3\n",
            "197 | 32 | 126 | 74 | 0\n",
            "198 | 32 | 137 | 18 | 3\n",
            "199 | 30 | 137 | 83 | 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "E. Network Traffic - Anomaly Detection"
      ],
      "metadata": {
        "id": "UD_7WsItdFgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Network_Traffic.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "XxBQovVXeg-C",
        "outputId": "36dbd91f-e8cf-4889-84e4-66c0cd88daf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   packet_size  inter_arrival_time  src_port  dst_port  packet_count_5s  \\\n",
              "0     0.405154            0.620362     62569       443         0.857143   \n",
              "1     0.527559            0.741288     59382       443         0.785714   \n",
              "2     0.226199            0.485116     65484        80         0.285714   \n",
              "3     0.573372            0.450965     51707        53         0.142857   \n",
              "4     0.651396            0.888740     26915        53         0.714286   \n",
              "\n",
              "   mean_packet_size  spectral_entropy  frequency_band_energy  label  \\\n",
              "0               0.0          0.834066               0.534891    0.0   \n",
              "1               0.0          0.147196               0.990757    0.0   \n",
              "2               0.0          0.855192               0.031781    0.0   \n",
              "3               0.0          0.153220               0.169958    0.0   \n",
              "4               0.0          0.923916               0.552053    0.0   \n",
              "\n",
              "   protocol_type_TCP  protocol_type_UDP  src_ip_192.168.1.2  \\\n",
              "0              False               True                True   \n",
              "1              False               True               False   \n",
              "2              False               True               False   \n",
              "3              False              False               False   \n",
              "4               True              False               False   \n",
              "\n",
              "   src_ip_192.168.1.3  dst_ip_192.168.1.5  dst_ip_192.168.1.6  tcp_flags_FIN  \\\n",
              "0               False               False               False          False   \n",
              "1               False               False                True          False   \n",
              "2               False                True               False          False   \n",
              "3                True               False               False          False   \n",
              "4                True               False               False          False   \n",
              "\n",
              "   tcp_flags_SYN  tcp_flags_SYN-ACK  \n",
              "0          False              False  \n",
              "1           True              False  \n",
              "2          False              False  \n",
              "3          False              False  \n",
              "4           True              False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e83601f-5326-45ff-a87d-a59ef2f557c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>packet_size</th>\n",
              "      <th>inter_arrival_time</th>\n",
              "      <th>src_port</th>\n",
              "      <th>dst_port</th>\n",
              "      <th>packet_count_5s</th>\n",
              "      <th>mean_packet_size</th>\n",
              "      <th>spectral_entropy</th>\n",
              "      <th>frequency_band_energy</th>\n",
              "      <th>label</th>\n",
              "      <th>protocol_type_TCP</th>\n",
              "      <th>protocol_type_UDP</th>\n",
              "      <th>src_ip_192.168.1.2</th>\n",
              "      <th>src_ip_192.168.1.3</th>\n",
              "      <th>dst_ip_192.168.1.5</th>\n",
              "      <th>dst_ip_192.168.1.6</th>\n",
              "      <th>tcp_flags_FIN</th>\n",
              "      <th>tcp_flags_SYN</th>\n",
              "      <th>tcp_flags_SYN-ACK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.405154</td>\n",
              "      <td>0.620362</td>\n",
              "      <td>62569</td>\n",
              "      <td>443</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.834066</td>\n",
              "      <td>0.534891</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.527559</td>\n",
              "      <td>0.741288</td>\n",
              "      <td>59382</td>\n",
              "      <td>443</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147196</td>\n",
              "      <td>0.990757</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.226199</td>\n",
              "      <td>0.485116</td>\n",
              "      <td>65484</td>\n",
              "      <td>80</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.855192</td>\n",
              "      <td>0.031781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.573372</td>\n",
              "      <td>0.450965</td>\n",
              "      <td>51707</td>\n",
              "      <td>53</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.153220</td>\n",
              "      <td>0.169958</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.651396</td>\n",
              "      <td>0.888740</td>\n",
              "      <td>26915</td>\n",
              "      <td>53</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.923916</td>\n",
              "      <td>0.552053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e83601f-5326-45ff-a87d-a59ef2f557c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e83601f-5326-45ff-a87d-a59ef2f557c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e83601f-5326-45ff-a87d-a59ef2f557c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4ac44113-2252-4e8c-b8aa-b045d2a12082\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ac44113-2252-4e8c-b8aa-b045d2a12082')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4ac44113-2252-4e8c-b8aa-b045d2a12082 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"packet_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2896058092368021,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 698,\n        \"samples\": [\n          0.5211166785969935,\n          0.1102362204724409,\n          0.7501789549033644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inter_arrival_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2811297281852635,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.1183769267142898,\n          0.9287212481377642,\n          0.4662182184852135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"src_port\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18520,\n        \"min\": 1038,\n        \"max\": 65484,\n        \"num_unique_values\": 995,\n        \"samples\": [\n          17995,\n          1348,\n          36764\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dst_port\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 180,\n        \"min\": 53,\n        \"max\": 443,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          443,\n          80,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"packet_count_5s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30327137112650976,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.5,\n          0.3571428571428571,\n          0.8571428571428571\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_packet_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spectral_entropy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2929266797243816,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.3432387889383932\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frequency_band_energy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29595284495022334,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.554030865876451\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30015011259383356,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protocol_type_TCP\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protocol_type_UDP\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"src_ip_192.168.1.2\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"src_ip_192.168.1.3\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dst_ip_192.168.1.5\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dst_ip_192.168.1.6\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tcp_flags_FIN\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tcp_flags_SYN\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tcp_flags_SYN-ACK\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV dan inspeksi awal\n",
        "if 'label' not in df.columns:\n",
        "    raise ValueError(\"Dataset harus punya kolom 'label' (0=normal, 1=anomaly).\")\n",
        "\n",
        "X = df.drop(columns=['label']).values.astype('float32')\n",
        "y = df['label'].values.astype('int32')"
      ],
      "metadata": {
        "id": "Xm5IUy3ygrW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Normalisasi\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(X)\n",
        "\n",
        "X_norm = normalizer(X)"
      ],
      "metadata": {
        "id": "7G5JQoGmgyx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Ambil hanya data normal untuk melatih Autoencoder\n",
        "X_train = X_norm[y == 0]\n",
        "\n",
        "print(f\"\\nData normal untuk training: {X_train.shape[0]} sampel\")\n",
        "print(f\"Data anomali untuk testing: {(y==1).sum()} sampel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkWlO1eWg_--",
        "outputId": "82256ad4-a331-431c-c346-c538680d3b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data normal untuk training: 900 sampel\n",
            "Data anomali untuk testing: 100 sampel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Bangun Autoencoder (TensorFlow)\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = input_dim // 2\n",
        "\n",
        "encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(input_dim,)),\n",
        "    tf.keras.layers.Dense(encoding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(encoding_dim // 2, activation='relu')\n",
        "])\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(encoding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "])\n",
        "\n",
        "autoencoder = tf.keras.Sequential([normalizer, encoder, decoder])\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "print(\"\\nModel summary (Autoencoder):\")\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "9bSx81hWhGQJ",
        "outputId": "4f775925-405b-4783-ff05-3369181ff7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model summary (Autoencoder):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_7 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;34m1000\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │            \u001b[38;5;34m35\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_4 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m180\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_5 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m215\u001b[0m (864.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215</span> (864.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m180\u001b[0m (720.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180</span> (720.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m35\u001b[0m (144.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35</span> (144.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train autoencoder dengan data normal\n",
        "history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=64, validation_split=0.2, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CXIYnnkhI9m",
        "outputId": "4332d2ca-e0df-4c23-a9a1-bb9346520b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "12/12 - 2s - 165ms/step - loss: 1.2093 - val_loss: 1.1715\n",
            "Epoch 2/20\n",
            "12/12 - 0s - 10ms/step - loss: 1.1893 - val_loss: 1.1529\n",
            "Epoch 3/20\n",
            "12/12 - 0s - 10ms/step - loss: 1.1699 - val_loss: 1.1324\n",
            "Epoch 4/20\n",
            "12/12 - 0s - 10ms/step - loss: 1.1474 - val_loss: 1.1068\n",
            "Epoch 5/20\n",
            "12/12 - 0s - 9ms/step - loss: 1.1202 - val_loss: 1.0758\n",
            "Epoch 6/20\n",
            "12/12 - 0s - 9ms/step - loss: 1.0885 - val_loss: 1.0411\n",
            "Epoch 7/20\n",
            "12/12 - 0s - 10ms/step - loss: 1.0547 - val_loss: 1.0064\n",
            "Epoch 8/20\n",
            "12/12 - 0s - 10ms/step - loss: 1.0224 - val_loss: 0.9761\n",
            "Epoch 9/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9953 - val_loss: 0.9527\n",
            "Epoch 10/20\n",
            "12/12 - 0s - 11ms/step - loss: 0.9748 - val_loss: 0.9360\n",
            "Epoch 11/20\n",
            "12/12 - 0s - 9ms/step - loss: 0.9604 - val_loss: 0.9248\n",
            "Epoch 12/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9506 - val_loss: 0.9173\n",
            "Epoch 13/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9437 - val_loss: 0.9121\n",
            "Epoch 14/20\n",
            "12/12 - 0s - 9ms/step - loss: 0.9387 - val_loss: 0.9084\n",
            "Epoch 15/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9347 - val_loss: 0.9055\n",
            "Epoch 16/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9315 - val_loss: 0.9031\n",
            "Epoch 17/20\n",
            "12/12 - 0s - 12ms/step - loss: 0.9288 - val_loss: 0.9011\n",
            "Epoch 18/20\n",
            "12/12 - 0s - 10ms/step - loss: 0.9264 - val_loss: 0.8992\n",
            "Epoch 19/20\n",
            "12/12 - 0s - 9ms/step - loss: 0.9241 - val_loss: 0.8974\n",
            "Epoch 20/20\n",
            "12/12 - 0s - 9ms/step - loss: 0.9219 - val_loss: 0.8957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Hitung reconstruction error untuk semua data\n",
        "reconstructions = autoencoder.predict(X_norm)\n",
        "errors = tf.reduce_mean(tf.math.squared_difference(X_norm, reconstructions), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeOstl6ehJq5",
        "outputId": "18188258-5282-4c4c-8004-867b5aefad39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Tentukan ambang (threshold)\n",
        "threshold = tf.math.reduce_mean(errors) + 2*tf.math.reduce_std(errors)\n",
        "print(f\"\\nAmbang batas error (threshold): {threshold:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCUEe2JhMUh",
        "outputId": "868d7900-0c05-45c1-deea-4d22dddb3a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ambang batas error (threshold): 1.225212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Label prediksi: > threshold = anomaly\n",
        "pred_labels = tf.cast(errors > threshold, tf.int32)"
      ],
      "metadata": {
        "id": "Xko-5kDKhO0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Evaluasi performa\n",
        "# pastikan y dan pred_labels adalah tf.Tensor bertipe int32\n",
        "# jika y adalah numpy array, jalankan dulu:\n",
        "# y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "tp = tf.reduce_sum(tf.cast((pred_labels == 1) & (y == 1), tf.int32))\n",
        "tn = tf.reduce_sum(tf.cast((pred_labels == 0) & (y == 0), tf.int32))\n",
        "fp = tf.reduce_sum(tf.cast((pred_labels == 1) & (y == 0), tf.int32))\n",
        "fn = tf.reduce_sum(tf.cast((pred_labels == 0) & (y == 1), tf.int32))\n",
        "\n",
        "# cast ke float sebelum pembagian\n",
        "tp_f = tf.cast(tp, tf.float32)\n",
        "fp_f = tf.cast(fp, tf.float32)\n",
        "fn_f = tf.cast(fn, tf.float32)\n",
        "\n",
        "precision = tp_f / (tp_f + fp_f + 1e-7)\n",
        "recall    = tp_f / (tp_f + fn_f + 1e-7)\n",
        "f1        = 2.0 * precision * recall / (precision + recall + 1e-7)\n",
        "\n",
        "# tampilkan hasil (konversi ke numpy untuk printing)\n",
        "print(\"\\nPerformance:\")\n",
        "print(f\" Precision: {precision.numpy():.4f}\")\n",
        "print(f\" Recall:    {recall.numpy():.4f}\")\n",
        "print(f\" F1-score:  {f1.numpy():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-QmGj1RhScF",
        "outputId": "d7edf9d2-dec8-43a0-9b6c-eb7744ecddfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance:\n",
            " Precision: 0.0714\n",
            " Recall:    0.0200\n",
            " F1-score:  0.0312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Visualisasi error distribusi\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(errors[y==0], bins=50, alpha=0.6, label='Normal')\n",
        "plt.hist(errors[y==1], bins=50, alpha=0.6, label='Anomaly')\n",
        "plt.axvline(threshold, color='red', linestyle='--', label='Threshold')\n",
        "plt.legend(); plt.title('Distribusi Reconstruction Error')\n",
        "plt.xlabel('Error'); plt.ylabel('Frekuensi')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "luFfnrNuhZRu",
        "outputId": "f0297973-7b64-401e-e889-9bad25ffa0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F. Heart Disease - Pattern Recognation"
      ],
      "metadata": {
        "id": "Xr1gOshuilrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # buka dialog upload\n",
        "\n",
        "\n",
        "# Ambil nama file yang diupload pertama\n",
        "csv_name = list(uploaded.keys())[0]\n",
        "print(f\"\\nFile yang diupload: {csv_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "IwpWekBhmuz6",
        "outputId": "b9e60679-cf70-40ba-8086-1565a66043f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-79049ee9-f4b7-4c29-a1d9-902e0d797e1a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-79049ee9-f4b7-4c29-a1d9-902e0d797e1a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving heart.csv to heart (3).csv\n",
            "\n",
            "File yang diupload: heart (3).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Load data\n",
        "df = pd.read_csv(csv_name)\n",
        "if 'target' not in df.columns:\n",
        "    raise ValueError(\"File harus mengandung kolom 'target' (0=sehat, 1=sakit).\")\n",
        "\n",
        "# one-hot untuk kolom non-numeric (sederhana)\n",
        "X = df.drop(columns=['target'])\n",
        "if X.select_dtypes(exclude=[np.number]).shape[1] > 0:\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "feature_names = list(X.columns)\n",
        "X = X.values.astype('float32')\n",
        "y = df['target'].values.astype('int32')\n",
        "\n",
        "if 'target' not in df.columns:\n",
        "    raise ValueError(\"Tidak menemukan kolom 'target'. Pastikan file memiliki kolom target (0 = sehat, 1 = sakit).\")\n",
        "\n",
        "print(\"\\nDistribusi label (target):\")\n",
        "print(df['target'].value_counts())\n",
        "plt.figure(figsize=(5,3))\n",
        "sns.countplot(x='target', data=df)\n",
        "plt.title('Distribusi target (0=Sehat, 1=Sakit)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "OOjKKG11ouFN",
        "outputId": "8eba90c8-815f-485d-d5a2-ca5362a87a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribusi label (target):\n",
            "target\n",
            "1    526\n",
            "0    499\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Split Fitur\n",
        "N = len(X)\n",
        "idx = np.arange(N); np.random.shuffle(idx)\n",
        "split = int(0.8 * N)\n",
        "train_idx, test_idx = idx[:split], idx[split:]\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "print(f\"Data split -> train: {X_train.shape}, test: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsEN-E3oo0Y4",
        "outputId": "7dd48da8-9950-44c4-83b8-ccb0e961ba1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split -> train: (820, 13), test: (205, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Normalizer\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(X_train)  # fit pada data train\n",
        "print(\"\\nNormalisasi: contoh mean/std dari 5 fitur pertama:\")\n",
        "means = normalizer.mean.numpy().ravel(); vars_ = normalizer.variance.numpy().ravel()\n",
        "stds = np.sqrt(vars_)\n",
        "for i in range(min(5, len(means))):\n",
        "    print(f\"  {i:02d}. {feature_names[i]:20s} mean={means[i]:.3f}, std={stds[i]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sueUEnBro28f",
        "outputId": "0d2ac3ab-7d9d-4429-dc31-946f7227aea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalisasi: contoh mean/std dari 5 fitur pertama:\n",
            "  00. age                  mean=54.470, std=9.040\n",
            "  01. sex                  mean=0.690, std=0.462\n",
            "  02. cp                   mean=0.952, std=1.038\n",
            "  03. trestbps             mean=131.652, std=17.811\n",
            "  04. chol                 mean=244.370, std=49.668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. tf.data pipeline\n",
        "BATCH = 32\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "ds_test  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "Dr0RIz1Co5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(X_train.shape[1],)),\n",
        "    normalizer,                               # preprocessing in-graph\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "], name='heart_simple_tf')\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "                       tf.keras.metrics.AUC(name='auc')])\n",
        "print(\"\\nModel summary:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "Ps9oa7Cfo7xN",
        "outputId": "b592633e-b779-47ee-d5ca-f46ca51d776e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"heart_simple_tf\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"heart_simple_tf\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_6 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │            \u001b[38;5;34m27\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ normalization_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,036\u001b[0m (11.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,036</span> (11.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,009\u001b[0m (11.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,009</span> (11.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m27\u001b[0m (112.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> (112.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train\n",
        "story = model.fit(ds_train, epochs=20, validation_data=ds_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g7xiawKBqKBS",
        "outputId": "597bf291-2b06-4707-b0ec-ac4b2a536a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "46/46 - 2s - 45ms/step - accuracy: 0.7323 - loss: 0.5635 - val_accuracy: 0.7475 - val_loss: 0.5139\n",
            "Epoch 2/30\n",
            "46/46 - 0s - 7ms/step - accuracy: 0.8405 - loss: 0.3960 - val_accuracy: 0.8081 - val_loss: 0.4547\n",
            "Epoch 3/30\n",
            "46/46 - 0s - 7ms/step - accuracy: 0.8571 - loss: 0.3464 - val_accuracy: 0.8081 - val_loss: 0.4372\n",
            "Epoch 4/30\n",
            "46/46 - 0s - 7ms/step - accuracy: 0.8752 - loss: 0.3258 - val_accuracy: 0.7980 - val_loss: 0.4278\n",
            "Epoch 5/30\n",
            "46/46 - 1s - 14ms/step - accuracy: 0.8835 - loss: 0.2938 - val_accuracy: 0.8081 - val_loss: 0.4149\n",
            "Epoch 6/30\n",
            "46/46 - 1s - 14ms/step - accuracy: 0.8793 - loss: 0.2872 - val_accuracy: 0.8283 - val_loss: 0.4037\n",
            "Epoch 7/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.8890 - loss: 0.2758 - val_accuracy: 0.8283 - val_loss: 0.3905\n",
            "Epoch 8/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.8877 - loss: 0.2660 - val_accuracy: 0.8283 - val_loss: 0.3755\n",
            "Epoch 9/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.8974 - loss: 0.2510 - val_accuracy: 0.8586 - val_loss: 0.3595\n",
            "Epoch 10/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.8974 - loss: 0.2423 - val_accuracy: 0.8687 - val_loss: 0.3462\n",
            "Epoch 11/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.8988 - loss: 0.2343 - val_accuracy: 0.8586 - val_loss: 0.3258\n",
            "Epoch 12/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.8988 - loss: 0.2295 - val_accuracy: 0.8788 - val_loss: 0.3164\n",
            "Epoch 13/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9098 - loss: 0.2138 - val_accuracy: 0.8889 - val_loss: 0.3002\n",
            "Epoch 14/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9112 - loss: 0.2023 - val_accuracy: 0.8889 - val_loss: 0.2877\n",
            "Epoch 15/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.9182 - loss: 0.1956 - val_accuracy: 0.8889 - val_loss: 0.2689\n",
            "Epoch 16/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9279 - loss: 0.1923 - val_accuracy: 0.8889 - val_loss: 0.2673\n",
            "Epoch 17/30\n",
            "46/46 - 0s - 7ms/step - accuracy: 0.9251 - loss: 0.1874 - val_accuracy: 0.8889 - val_loss: 0.2481\n",
            "Epoch 18/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.9168 - loss: 0.1858 - val_accuracy: 0.9091 - val_loss: 0.2311\n",
            "Epoch 19/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.9334 - loss: 0.1679 - val_accuracy: 0.8990 - val_loss: 0.2249\n",
            "Epoch 20/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9320 - loss: 0.1676 - val_accuracy: 0.9091 - val_loss: 0.2102\n",
            "Epoch 21/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.9279 - loss: 0.1593 - val_accuracy: 0.9091 - val_loss: 0.1993\n",
            "Epoch 22/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9404 - loss: 0.1452 - val_accuracy: 0.9091 - val_loss: 0.1895\n",
            "Epoch 23/30\n",
            "46/46 - 0s - 5ms/step - accuracy: 0.9390 - loss: 0.1482 - val_accuracy: 0.9495 - val_loss: 0.1870\n",
            "Epoch 24/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9459 - loss: 0.1438 - val_accuracy: 0.9394 - val_loss: 0.1679\n",
            "Epoch 25/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9459 - loss: 0.1494 - val_accuracy: 0.9293 - val_loss: 0.1736\n",
            "Epoch 26/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9584 - loss: 0.1254 - val_accuracy: 0.9495 - val_loss: 0.1656\n",
            "Epoch 27/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9598 - loss: 0.1381 - val_accuracy: 0.9192 - val_loss: 0.1576\n",
            "Epoch 28/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9570 - loss: 0.1287 - val_accuracy: 0.9495 - val_loss: 0.1458\n",
            "Epoch 29/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9667 - loss: 0.1109 - val_accuracy: 0.9495 - val_loss: 0.1324\n",
            "Epoch 30/30\n",
            "46/46 - 0s - 4ms/step - accuracy: 0.9653 - loss: 0.1190 - val_accuracy: 0.9495 - val_loss: 0.1295\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluasi pada test set\n",
        "\n",
        "loss, acc, auc = model.evaluate(ds_test, verbose=0)\n",
        "print(f\"\\nTest loss={loss:.4f} | test accuracy={acc:.4f} | test AUC={auc:.4f}\")\n",
        "\n",
        "# Prediksi probabilitas -> numpy\n",
        "y_prob = model.predict(ds_test).ravel()\n",
        "y_pred = (y_prob >= 0.5).astype('int32')\n",
        "\n",
        "# Confusion matrix via TF -> numpy\n",
        "cm = tf.math.confusion_matrix(y_test, y_pred, num_classes=2).numpy()\n",
        "print(\"\\nConfusion matrix (rows=actual, cols=predicted):\")\n",
        "print(cm)\n",
        "\n",
        "# Precision, recall, f1 (manual)\n",
        "tp = int(cm[1,1]); tn = int(cm[0,0]); fp = int(cm[0,1]); fn = int(cm[1,0])\n",
        "precision = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
        "recall = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
        "f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
        "print(f\"\\nPrecision={precision:.4f} | Recall={recall:.4f} | F1={f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jx54QW3qPii",
        "outputId": "9fa5f392-2c3e-4bc1-d299-758d1b2d8b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test loss=0.6405 | test accuracy=0.5951 | test AUC=0.7986\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\n",
            "Confusion matrix (rows=actual, cols=predicted):\n",
            "[[98  2]\n",
            " [81 24]]\n",
            "\n",
            "Precision=0.9231 | Recall=0.2286 | F1=0.3664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Contoh prediksi\n",
        "print(\"\\nContoh prediksi (prob, pred, true) — 8 sampel pertama dari test set:\")\n",
        "for i in range(min(8, len(y_test))):\n",
        "    print(f\" idx{i:02d}: prob={y_prob[i]:.3f} pred={y_pred[i]} true={y_test[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcm1hJrfqSJs",
        "outputId": "4b6363bc-175e-4e4d-c97a-772b2b7cb841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh prediksi (prob, pred, true) — 8 sampel pertama dari test set:\n",
            " idx00: prob=0.387 pred=0 true=1\n",
            " idx01: prob=0.549 pred=1 true=1\n",
            " idx02: prob=0.366 pred=0 true=0\n",
            " idx03: prob=0.374 pred=0 true=0\n",
            " idx04: prob=0.328 pred=0 true=0\n",
            " idx05: prob=0.358 pred=0 true=0\n",
            " idx06: prob=0.530 pred=1 true=0\n",
            " idx07: prob=0.443 pred=0 true=1\n"
          ]
        }
      ]
    }
  ]
}